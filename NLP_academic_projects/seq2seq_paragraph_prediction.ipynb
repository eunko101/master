{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## This codework is one of the project of the course \"Machine Leaning for NLP3\" of University of Paris Cité. The strutre of the code was given by professor of the course, M. Bernard Timothé. I and my teammate, Irene completed the some parts between lines of hashtags (##########) including essential part of training and prediciton of the model."
      ],
      "metadata": {
        "id": "N6xevXswbsPr"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTLyS0rZh9Ql"
      },
      "source": [
        "Goal\n",
        "==\n",
        "We are about to train a *sequence-to-sequence model* to predict a paragraph of Gustave Flaubert's *Madame Bovary* given the preceding paragraph.\n",
        "The model (at least in its first version) does not use words as units of text but characters.\n",
        "\n",
        "*   The encoder part, based on a bidirectional LSTM, reads an input paragraph and turns it into a set of tensors that serves as initial state for the decoder part.\n",
        "*   The decoder part is based on an (unidirectional) LSTM. The state of the LSTM is used to compute a probability distribution over the alphabet (including space and punctuation marks) and is updated each time a character is predicted by the LSTM reading this character's embedding.\n",
        "*   The goal is to get the best model. It is part of the job to define what this means. It is also part of the job to explain me how you get your best model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An16FNHuhZI1"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "5Z-i_eHvbL61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed520e02-70a4-4541-b552-df9f62caf1ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TK2oBAYuiZPX"
      },
      "source": [
        "Downloading the dataset\n",
        "==\n",
        "The dataset we are going to use is there: \"https://www.gutenberg.org/cache/epub/14155/pg14155.txt\"\n",
        "\n",
        "We have to pre-process it a little bit in order to remove everything that is not part of the text and to split the actual text into paragraphs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## toy data set"
      ],
      "metadata": {
        "id": "KLr95q-Yuv_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "use_toy_dataset = False # If True, a toy dataset (see below) is use instead of the real one.\n",
        "# use_toy_dataset = True # If True, a toy dataset (see below) is use instead of the real one."
      ],
      "metadata": {
        "id": "veOPiPCIlOOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VXDpa1tiSfQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e271b86-4c69-4bd8-cbd9-1a03d813d507"
      },
      "source": [
        "# Downloads the dataset.\n",
        "import urllib\n",
        "\n",
        "tmp = urllib.request.urlretrieve(\"https://www.gutenberg.org/cache/epub/14155/pg14155.txt\")\n",
        "filename = tmp[0]\n",
        "print(filename)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmptnonkf6c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Downloads the dataset.\n",
        "another_file = urllib.request.urlretrieve(\"https://www.gutenberg.org/cache/epub/14157/pg14157.txt\")\n",
        "another_filename = another_file[0]\n",
        "print(another_filename)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5rOie6UJXcl",
        "outputId": "2faa7ced-169b-45da-c9ee-3e364c5d1f40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/tmpylz04f_4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU4hirpsiWX2"
      },
      "source": [
        "# Prints the first 200 lines in the file with their line number.\n",
        "# This shows that we have a little bit of preprocessing to do in order to clean the data.\n",
        "with open(filename) as f:\n",
        "  for i in range(200):\n",
        "    print(f\"[{i}] {f.readline()}\", end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re # Regular expression library\n",
        "roman_regex = re.compile('^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$') # This regular expression matches Roman numerals but also the empty string.\n",
        "\n",
        "EOP = '\\n' # The end-of-line character will be used to mark the end of paragraphs.\n",
        "\n",
        "\n",
        "with open(filename) as f:\n",
        "  # We want to skip everything before the actual text of the novel.\n",
        "  # The line \"PREMIÈRE PARTIE\" appears twice: in the table of content and then at the start of the first part of the actual text.\n",
        "  # The following lines discard everything up to this second occurence (included).\n",
        "  skip = 2\n",
        "  while(skip > 0):\n",
        "    line = f.readline().strip()\n",
        "    if(line == \"PREMIÈRE PARTIE\"): skip -= 1;\n",
        "\n",
        "  paragraphs = [] # Note that each dialog line will be considered a separate paragraph.\n",
        "  paragraph_buffer = [] # List[str]; each element corresponds to a line in the original text file + an additonal space if necessary.\n",
        "  while(True):\n",
        "    line = f.readline().strip()\n",
        "    if(\"END OF THE PROJECT GUTENBERG EBOOK MADAME BOVARY\" in line): break # End of the actual text.\n",
        "\n",
        "    if(line == \"\"): # We've reached the end of a paragraph.\n",
        "      if(len(paragraph_buffer) > 0):\n",
        "        paragraph_buffer.append(EOP) # End of the paragraph.\n",
        "\n",
        "        paragraph = \"\".join(paragraph_buffer) # The different lines that make up the paragraph are joined into a single string.\n",
        "        paragraphs.append(paragraph)\n",
        "        paragraph_buffer = []\n",
        "      continue\n",
        "\n",
        "    if(roman_regex.match(line)): continue # Ignores the lines that indicate the beginning of a chapter.\n",
        "    if(line.endswith(\" PARTIE\")): continue # Ignores the lines that indicate the beginning of a part.\n",
        "\n",
        "    if((len(paragraph_buffer) > 0) and (paragraph_buffer[-1][-1] != '-')):\n",
        "      paragraph_buffer.append(' ')\n",
        "      '''Adds a space between consecutive lines except when the first one ends with \"-\"\n",
        "       (e.g. if the word \"pomme-de-terre\" is split with \"pomme-de-\" at the end of a line and \"terre\" at the beginning of the next,\n",
        "        we do not want to join the two lines with a space).'''\n",
        "    paragraph_buffer.append(line)\n",
        "\n",
        "paragraphs1 = paragraphs\n",
        "# print(f\"{len(paragraphs)} paragraphs read.\")\n",
        "# for i in range(3): print(paragraphs[i], end='')"
      ],
      "metadata": {
        "id": "FNBBth0TNg7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctl4Z9Gti-6U"
      },
      "source": [
        "import re # Regular expression library\n",
        "roman_regex = re.compile('^M{0,4}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$') # This regular expression matches Roman numerals but also the empty string.\n",
        "\n",
        "EOP = '\\n' # The end-of-line character will be used to mark the end of paragraphs.\n",
        "\n",
        "\n",
        "with open(another_filename) as f:\n",
        "  # We want to skip everything before the actual text of the novel.\n",
        "  # The line \"PREMIÈRE PARTIE\" appears twice: in the table of content and then at the start of the first part of the actual text.\n",
        "  # The following lines discard everything up to this second occurence (included).\n",
        "  skip = 2\n",
        "  while(skip > 0):\n",
        "    line = f.readline().strip()\n",
        "    if(line == \"CHAPITRE I\"): skip -= 1;\n",
        "\n",
        "\n",
        "  paragraphs2 = [] # Note that each dialog line will be considered a separate paragraph.\n",
        "  paragraph_buffer2 = [] # List[str]; each element corresponds to a line in the original text file + an additonal space if necessary.\n",
        "  while(True):\n",
        "    line = f.readline().strip()\n",
        "    if(\"END OF THE PROJECT GUTENBERG EBOOK BOUVARD ET PÉCUCHET\" in line): break # End of the actual text.\n",
        "\n",
        "    if(line == \"\"): # We've reached the end of a paragraph.\n",
        "      if(len(paragraph_buffer2) > 0):\n",
        "        paragraph_buffer2.append(EOP) # End of the paragraph.\n",
        "\n",
        "        paragraph = \"\".join(paragraph_buffer2) # The different lines that make up the paragraph are joined into a single string.\n",
        "        paragraphs2.append(paragraph)\n",
        "        paragraph_buffer2 = []\n",
        "      continue\n",
        "\n",
        "    if(roman_regex.match(line)): continue # Ignores the lines that indicate the beginning of a chapter.\n",
        "    if(line.endswith(\" PARTIE\")): continue # Ignores the lines that indicate the beginning of a part.\n",
        "\n",
        "    if((len(paragraph_buffer2) > 0) and (paragraph_buffer2[-1][-1] != '-')):\n",
        "      paragraph_buffer2.append(' ')\n",
        "      '''Adds a space between consecutive lines except when the first one ends with \"-\"\n",
        "       (e.g. if the word \"pomme-de-terre\" is split with \"pomme-de-\" at the end of a line and \"terre\" at the beginning of the next,\n",
        "        we do not want to join the two lines with a space).'''\n",
        "    paragraph_buffer2.append(line)\n",
        "\n",
        "# print(f\"{len(paragraphs2)} paragraphs read.\")\n",
        "# for i in range(30): print(paragraphs2[i], end='')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paragraphs = paragraphs1 + paragraphs2\n",
        "len(paragraphs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTZ-THaCSye-",
        "outputId": "6a4e601f-455c-42e3-80ed-74b80906f25a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6278"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## split"
      ],
      "metadata": {
        "id": "y14uOtJAxJD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import random\n",
        "# random.shuffle(paragraphs)\n",
        "# dataset = {\"train\":[], \"dev\":[], \"test\":[]}\n",
        "# dataset[\"train\"] = paragraphs[:(int(len(paragraphs)*0.7))]\n",
        "# dataset[\"dev\"] = paragraphs[int(len(paragraphs)*0.7):int(len(paragraphs)*0.9)]\n",
        "# dataset[\"test\"] = paragraphs[int(len(paragraphs)*0.9):]\n",
        "# for part in dataset.keys(): print(len(dataset[part]))"
      ],
      "metadata": {
        "id": "mGErsMLvxNF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we define a toy dataset on which your model, if correctly implemented, should be able to learn more easily."
      ],
      "metadata": {
        "id": "QZttAP5uj6y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use_toy_dataset = False"
      ],
      "metadata": {
        "id": "g8H37BT7exEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if(use_toy_dataset):\n",
        "  paragraphs = []\n",
        "\n",
        "  import random, string\n",
        "  characters = list(string.ascii_lowercase + string.ascii_lowercase.upper() + \"_-/\\'[]()\")\n",
        "  # print(\"characters\", characters) #len 60\n",
        "  random.shuffle(characters)\n",
        "  k = random.randint(1, 10)\n",
        "  a = \"a\"\n",
        "  paragraph = (a * k)\n",
        "  for _ in range(100):\n",
        "    random.shuffle(characters)\n",
        "    for a in characters:\n",
        "      k = random.randint(1, 16)\n",
        "      paragraph += f\"? Now, please write {k} {a}.{EOP}\"\n",
        "      paragraphs.append(paragraph)\n",
        "      paragraph = (a * k)\n",
        "  print(f\"{len(paragraphs)} paragraphs generated.\")\n",
        "\n",
        "  print(paragraphs[:10])"
      ],
      "metadata": {
        "id": "28V5KvT6jUFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAn1SqQE1Hr-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80aae63d-be05-48c9-961d-112a2ee9e817"
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "\n",
        "# Computes the frequency of all characters in the dataset.\n",
        "char_counts = collections.defaultdict(int)\n",
        "for paragraph in paragraphs:\n",
        "  for char in paragraph: char_counts[char] += 1\n",
        "\n",
        "print(f\"{len(char_counts)} different characters found in the dataset.\") # 60 becomes 75 because of numbers added (randint of k)\n",
        "print(sorted(char_counts.items(), key=(lambda x: x[1]), reverse=True)) # Shows each character with its frequency, in decreasing frequency order."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 different characters found in the dataset.\n",
            "[(' ', 191850), ('e', 137585), ('a', 78879), ('s', 76996), ('t', 69896), ('i', 68086), ('n', 64638), ('r', 62894), ('u', 59983), ('l', 58562), ('o', 49667), ('d', 35130), ('c', 27866), ('m', 25786), ('p', 24691), (',', 20222), ('é', 15719), ('v', 15206), (\"'\", 12853), ('.', 11559), ('b', 9909), ('f', 9776), ('h', 9723), ('-', 9216), ('q', 8913), ('g', 8868), ('\\n', 6278), ('à', 4273), ('x', 4010), ('è', 3538), ('j', 2906), ('!', 2901), ('y', 2896), (';', 2317), ('L', 2188), ('ê', 2109), ('E', 2090), ('C', 1750), ('P', 1611), ('I', 1589), ('M', 1544), ('B', 1405), ('z', 1159), ('A', 1062), ('?', 1007), ('D', 862), (':', 853), ('â', 779), ('ç', 749), ('S', 691), ('î', 583), ('O', 574), ('R', 515), ('V', 497), ('ô', 493), ('J', 485), ('T', 484), ('Q', 455), ('ù', 430), ('H', 415), ('û', 388), ('U', 347), ('N', 338), ('F', 327), ('G', 292), ('_', 160), ('À', 155), ('«', 126), ('É', 116), ('»', 115), ('ï', 90), ('Y', 86), ('(', 82), (')', 82), ('1', 58), ('k', 54), ('X', 39), ('8', 26), ('2', 23), ('3', 21), ('4', 21), ('9', 19), ('5', 18), ('ë', 17), ('Ç', 17), ('w', 13), ('K', 13), ('0', 12), ('W', 8), ('7', 8), ('6', 8), ('ü', 7), ('Z', 7), ('Ê', 6), ('°', 4), ('Î', 1), ('È', 1), ('=', 1), ('ñ', 1), ('º', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nF0YFZTxD_A0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb4d344a-7079-4ab1-999a-00b92d829e29"
      },
      "source": [
        "# Here you have to build a dictionary 'char_vocabulary' that assigns an integer id to each character,\n",
        "# along with a list/array 'id_to_char' that implements the reverse mapping.\n",
        "#################\n",
        "id_to_char = list(char_counts.keys())\n",
        "char_vocabulary = { ch : idx for idx, ch in list(enumerate(id_to_char))}\n",
        "print(\"id_to_char\", id_to_char)\n",
        "print(\"char_vocabulary\", char_vocabulary)\n",
        "#################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id_to_char ['N', 'o', 'u', 's', ' ', 'é', 't', 'i', 'n', 'à', 'l', \"'\", 'É', 'd', 'e', ',', 'q', 'a', 'P', 'r', 'v', 'h', 'b', 'g', 'ç', 'c', 'p', '.', 'C', 'x', 'm', 'è', '\\n', 'L', 'f', ';', 'î', ':', '-', 'M', 'R', 'j', 'S', 'ù', 'â', 'z', 'I', 'Q', 'û', 'ê', 'O', 'y', 'k', 'ï', 'E', 'T', 'U', 'D', '!', '(', ')', 'B', '?', '_', 'G', '1', '8', '2', 'A', 'À', 'H', 'ô', 'V', 'Ê', '«', '»', 'Y', 'F', 'J', 'ë', 'W', 'X', '0', '5', '7', '9', '6', '3', 'w', 'Î', 'Ç', '4', 'ü', '°', 'K', 'Z', 'È', '=', 'ñ', 'º']\n",
            "char_vocabulary {'N': 0, 'o': 1, 'u': 2, 's': 3, ' ': 4, 'é': 5, 't': 6, 'i': 7, 'n': 8, 'à': 9, 'l': 10, \"'\": 11, 'É': 12, 'd': 13, 'e': 14, ',': 15, 'q': 16, 'a': 17, 'P': 18, 'r': 19, 'v': 20, 'h': 21, 'b': 22, 'g': 23, 'ç': 24, 'c': 25, 'p': 26, '.': 27, 'C': 28, 'x': 29, 'm': 30, 'è': 31, '\\n': 32, 'L': 33, 'f': 34, ';': 35, 'î': 36, ':': 37, '-': 38, 'M': 39, 'R': 40, 'j': 41, 'S': 42, 'ù': 43, 'â': 44, 'z': 45, 'I': 46, 'Q': 47, 'û': 48, 'ê': 49, 'O': 50, 'y': 51, 'k': 52, 'ï': 53, 'E': 54, 'T': 55, 'U': 56, 'D': 57, '!': 58, '(': 59, ')': 60, 'B': 61, '?': 62, '_': 63, 'G': 64, '1': 65, '8': 66, '2': 67, 'A': 68, 'À': 69, 'H': 70, 'ô': 71, 'V': 72, 'Ê': 73, '«': 74, '»': 75, 'Y': 76, 'F': 77, 'J': 78, 'ë': 79, 'W': 80, 'X': 81, '0': 82, '5': 83, '7': 84, '9': 85, '6': 86, '3': 87, 'w': 88, 'Î': 89, 'Ç': 90, '4': 91, 'ü': 92, '°': 93, 'K': 94, 'Z': 95, 'È': 96, '=': 97, 'ñ': 98, 'º': 99}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C71VlK5e3Gg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66c329a7-c2cc-4e4f-e138-57dbcce37c69"
      },
      "source": [
        "EOP_id = char_vocabulary[EOP] # Id for the end-of-paragraph symbol\n",
        "\n",
        "print(char_vocabulary)\n",
        "print(id_to_char)\n",
        "print(f\"EOP_id = {EOP_id}\")\n",
        "\n",
        "# Here you have to implement a test that proves that your implementations of 'char_vocabulary' and 'id_to_char' are consistent.\n",
        "#################\n",
        "# iterating on char is is more elegant and solid way to test\n",
        "for ch in id_to_char:\n",
        "  assert char_vocabulary[ch] == char_vocabulary[id_to_char[char_vocabulary[ch]]]\n",
        "print(\"Every mapping of char and id is consistent.\")\n",
        "\n",
        "\n",
        "#################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'N': 0, 'o': 1, 'u': 2, 's': 3, ' ': 4, 'é': 5, 't': 6, 'i': 7, 'n': 8, 'à': 9, 'l': 10, \"'\": 11, 'É': 12, 'd': 13, 'e': 14, ',': 15, 'q': 16, 'a': 17, 'P': 18, 'r': 19, 'v': 20, 'h': 21, 'b': 22, 'g': 23, 'ç': 24, 'c': 25, 'p': 26, '.': 27, 'C': 28, 'x': 29, 'm': 30, 'è': 31, '\\n': 32, 'L': 33, 'f': 34, ';': 35, 'î': 36, ':': 37, '-': 38, 'M': 39, 'R': 40, 'j': 41, 'S': 42, 'ù': 43, 'â': 44, 'z': 45, 'I': 46, 'Q': 47, 'û': 48, 'ê': 49, 'O': 50, 'y': 51, 'k': 52, 'ï': 53, 'E': 54, 'T': 55, 'U': 56, 'D': 57, '!': 58, '(': 59, ')': 60, 'B': 61, '?': 62, '_': 63, 'G': 64, '1': 65, '8': 66, '2': 67, 'A': 68, 'À': 69, 'H': 70, 'ô': 71, 'V': 72, 'Ê': 73, '«': 74, '»': 75, 'Y': 76, 'F': 77, 'J': 78, 'ë': 79, 'W': 80, 'X': 81, '0': 82, '5': 83, '7': 84, '9': 85, '6': 86, '3': 87, 'w': 88, 'Î': 89, 'Ç': 90, '4': 91, 'ü': 92, '°': 93, 'K': 94, 'Z': 95, 'È': 96, '=': 97, 'ñ': 98, 'º': 99}\n",
            "['N', 'o', 'u', 's', ' ', 'é', 't', 'i', 'n', 'à', 'l', \"'\", 'É', 'd', 'e', ',', 'q', 'a', 'P', 'r', 'v', 'h', 'b', 'g', 'ç', 'c', 'p', '.', 'C', 'x', 'm', 'è', '\\n', 'L', 'f', ';', 'î', ':', '-', 'M', 'R', 'j', 'S', 'ù', 'â', 'z', 'I', 'Q', 'û', 'ê', 'O', 'y', 'k', 'ï', 'E', 'T', 'U', 'D', '!', '(', ')', 'B', '?', '_', 'G', '1', '8', '2', 'A', 'À', 'H', 'ô', 'V', 'Ê', '«', '»', 'Y', 'F', 'J', 'ë', 'W', 'X', '0', '5', '7', '9', '6', '3', 'w', 'Î', 'Ç', '4', 'ü', '°', 'K', 'Z', 'È', '=', 'ñ', 'º']\n",
            "EOP_id = 32\n",
            "Every mapping of char and id is consistent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bc3Gst0zhQv"
      },
      "source": [
        "# Turns a list of lists of ids into a list of strings.\n",
        "# Do not forget that an occurrence of EOP means that the paragraph ends here.\n",
        "def ids_to_texts(ids):\n",
        "  # Here you have to turn each list of character ids of 'ids' into a string and then return all strings as a list.\n",
        "  #################\n",
        "  res = \"\"\n",
        "  fin = []\n",
        "  for para_id in ids :\n",
        "    for char_id in para_id :\n",
        "      if char_id != char_vocabulary[EOP]:\n",
        "        res += id_to_char[char_id]\n",
        "      else:\n",
        "        break\n",
        "    fin.append(res)\n",
        "    res = \"\"\n",
        "  return fin\n",
        "\n",
        "  #################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybAhzb4_3RTk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e0d891-7906-4d51-8203-8b4ac32a2f32"
      },
      "source": [
        "ps = [\"Bonjour.\", \"Comment allez vous ?\"]\n",
        "ids = [[char_vocabulary[c] for c in p] for p in ps]\n",
        "print(\"ids : \", ids)\n",
        "print( \"ids_to_texts : \", ids_to_texts(ids))\n",
        "print(f\"'ids_to_texts(ids) == ps' should be True: {ids_to_texts(ids) == ps}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ids :  [[61, 1, 8, 41, 1, 2, 19, 27], [28, 1, 30, 30, 14, 8, 6, 4, 17, 10, 10, 14, 45, 4, 20, 1, 2, 3, 4, 62]]\n",
            "ids_to_texts :  ['Bonjour.', 'Comment allez vous ?']\n",
            "'ids_to_texts(ids) == ps' should be True: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnT7T-yHEPCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a080dc25-7d2f-4de6-b47a-35602bbffe8d"
      },
      "source": [
        "ps = [\"Bonjour.\", \"Comment allez vous ?\"]\n",
        "ids = [[char_vocabulary[c] for c in p] for p in ps]\n",
        "print(ids)\n",
        "ids[0].extend([EOP_id, (EOP_id+1), (EOP_id+1)]) # With the end-of-paragraph token id and additional (padding-like) stuff for the first string.\n",
        "print(ids)\n",
        "print(ids_to_texts(ids))\n",
        "print(f\"'ids_to_texts(ids) == ps' should be True: {ids_to_texts(ids) == ps}\")\n",
        "# If you have a problem here, remember that EOP indicates the end of the text (this might be related to your problem)."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[61, 1, 8, 41, 1, 2, 19, 27], [28, 1, 30, 30, 14, 8, 6, 4, 17, 10, 10, 14, 45, 4, 20, 1, 2, 3, 4, 62]]\n",
            "[[61, 1, 8, 41, 1, 2, 19, 27, 32, 33, 33], [28, 1, 30, 30, 14, 8, 6, 4, 17, 10, 10, 14, 45, 4, 20, 1, 2, 3, 4, 62]]\n",
            "['Bonjour.', 'Comment allez vous ?']\n",
            "'ids_to_texts(ids) == ps' should be True: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBuCiGqO08nd"
      },
      "source": [
        "Batch generator\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aoH4g-Fkkrgc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b5c5c1a-a7e6-4cce-f51f-a3f7935cae13"
      },
      "source": [
        "# Defines a class of objects that produce batches from the dataset.\n",
        "# A training instance is composed of a pair of consecutive paragraphs.\n",
        "# The goal will be to predict the second given the first.\n",
        "# (Possible improvement: As is, ends of chapter are completely ignored:\n",
        "# the last paragraph of a chapter and the first of the following chapter form a training instance.\n",
        "# We might want to predict the end of the chapter instead, or simply remove these pairs from the dataset.)\n",
        "\n",
        "class BatchGenerator:\n",
        "  def __init__(self, paragraphs, char_vocabulary):\n",
        "    self.paragraphs = paragraphs\n",
        "    self.char_vocabulary = char_vocabulary # Dictionary\n",
        "    self.padding_idx = len(char_vocabulary)\n",
        "\n",
        "  # Returns the number of training instances (i.e. of pairs of consecutive paragraphs).\n",
        "  def length(self):\n",
        "    return (len(self.paragraphs) - 1)\n",
        "\n",
        "  # Returns a random training batch (composed of pairs of consecutive paragraphs).\n",
        "  # If `subset` is an integer, only a subset of the corpus is used. This can be useful when debugging the system.\n",
        "  def get_batch(self, batch_size, subset=None):\n",
        "    max_i = self.length() if(subset is None) else min(subset, self.length())\n",
        "    paragraph_ids = np.random.randint(max_i, size=batch_size) # Randomly picks some paragraph ids.\n",
        "\n",
        "    return self._ids_to_batch(paragraph_ids)\n",
        "\n",
        "  def _ids_to_batch(self, paragraph_ids):\n",
        "    firsts = [] # First paragraph of each pair\n",
        "    seconds = [] # Second paragraph of each pair\n",
        "    for paragraph_id in paragraph_ids:\n",
        "      firsts.append([self.char_vocabulary[char] for char in self.paragraphs[paragraph_id]])\n",
        "      seconds.append([self.char_vocabulary[char] for char in self.paragraphs[paragraph_id + 1]])\n",
        "\n",
        "    # Padding\n",
        "    self.pad(firsts)\n",
        "    self.pad(seconds)\n",
        "\n",
        "    firsts = torch.tensor(firsts, dtype=torch.long) # Conversion to a tensor\n",
        "    seconds = torch.tensor(seconds, dtype=torch.long) # Conversion to a tensor\n",
        "\n",
        "    return (firsts, seconds)\n",
        "\n",
        "  # Pads a list of lists (i.e. adds fake word ids so that all sequences in the batch have the same length, so that we can use a matrix to represent them).\n",
        "  # In place\n",
        "  def pad(self, sequences):\n",
        "    max_length = max([len(s) for s in sequences])\n",
        "    for s in sequences: s.extend([self.padding_idx] * (max_length - len(s)))\n",
        "\n",
        "  # Returns a generator of training batches for a full epoch.\n",
        "  #  (Note that this function is not used in the training loop implemented below. `get_batch` is used instead.)\n",
        "  # If `subset` is an integer, only a subset of the corpus is used. This can be useful when debugging the system.\n",
        "  def all_batches(self, batch_size, subset=None):\n",
        "    max_i = self.length() if(subset is None) else min(subset, self.length())\n",
        "\n",
        "    # Loop that generates all full batches (batches of size 'batch_size').\n",
        "    i = 0\n",
        "    while((i + batch_size) <= max_i):\n",
        "      instance_ids = np.arange(i, (i + batch_size)) #numbers from i to (i+bsize)\n",
        "      yield self._ids_to_batch(instance_ids)\n",
        "      i += batch_size\n",
        "\n",
        "    # Possibly generates the last (not full) batch.\n",
        "    if(i < max_i):\n",
        "      instance_ids = np.arange(i, max_i)\n",
        "      yield self._ids_to_batch(instance_ids)\n",
        "\n",
        "  # Turns a list of arbitrary paragraphs into a prediction batch.\n",
        "  def turn_into_batch(self, paragraphs):\n",
        "    firsts = []\n",
        "    for paragraph in paragraphs:\n",
        "        # Unknown characters are ignored (removed).\n",
        "        tmp = []\n",
        "        for char in paragraph:\n",
        "          if(char in self.char_vocabulary): tmp.append(self.char_vocabulary[char])\n",
        "\n",
        "        if(tmp[-1] != EOP_id): tmp.append(EOP_id) # Adds an end-of-paragraph character if necessary.\n",
        "\n",
        "        firsts.append(tmp)\n",
        "\n",
        "    self.pad(firsts)\n",
        "    return torch.tensor(firsts, dtype=torch.long)\n",
        "\n",
        "batch_generator = BatchGenerator(paragraphs=paragraphs, char_vocabulary=char_vocabulary)\n",
        "# batch_generator = BatchGenerator(paragraphs=['Bonjour.\\n', 'Salutoioiiiii\\n', 'Hello\\n', 'World\\n', 'Bye\\n', 'Thanks\\n'], char_vocabulary=char_vocabulary)\n",
        "# train_batch_generator = BatchGenerator(paragraphs=dataset[\"train\"], char_vocabulary=char_vocabulary)\n",
        "\n",
        "print(batch_generator.length())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6277\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# batch_generator.paragraphs\n",
        "# batch_generator.padding_idx"
      ],
      "metadata": {
        "id": "YdNra0LNZgti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(firsts, seconds) = batch_generator.get_batch(3)\n",
        "print(ids_to_texts(firsts))\n",
        "print(ids_to_texts(seconds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3SlHsRMCLOG",
        "outputId": "307b567c-df0b-450a-92d7-899ddf5c6a63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"-- J'ai été chez trois personnes... inutilement!\", 'Il reparut avec elle, et un Molière.', \"Le notaire l'arrêta; et se haussant à chaque mot sur la pointe des orteils:\"]\n",
            "[\"Puis ils restèrent assis l'un en face de l'autre, aux deux coins de la cheminée, immobiles, sans parler. Emma haussait les épaules, tout en trépignant. Il l'entendit qui murmurait:\", \"Le commencement fut médiocre. Mais Tartuffe venant à caresser les genoux d'Elmire, Pécuchet prit un ton de gendarme.\", \"--Je trouve votre système d'une immoralité complète. Il donne carrière à tous les débordements, excuse les crimes, innocente les coupables.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = ids_to_texts(firsts)\n",
        "print(\"test :\", test)\n",
        "batch = batch_generator.turn_into_batch(test)\n",
        "print(\"batch : \", batch) # (B, seq_len)\n",
        "text = ids_to_texts(batch)\n",
        "print(\"back to text : \", text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKK6gGjO88Rk",
        "outputId": "4cf239ec-5938-4b76-eb76-e6e1b0fe8218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test : [\"-- J'ai été chez trois personnes... inutilement!\", 'Il reparut avec elle, et un Molière.', \"Le notaire l'arrêta; et se haussant à chaque mot sur la pointe des orteils:\"]\n",
            "batch :  tensor([[ 38,  38,   4,  78,  11,  17,   7,   4,   5,   6,   5,   4,  25,  21,\n",
            "          14,  45,   4,   6,  19,   1,   7,   3,   4,  26,  14,  19,   3,   1,\n",
            "           8,   8,  14,   3,  27,  27,  27,   4,   7,   8,   2,   6,   7,  10,\n",
            "          14,  30,  14,   8,   6,  58,  32, 100, 100, 100, 100, 100, 100, 100,\n",
            "         100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
            "         100, 100, 100, 100, 100, 100],\n",
            "        [ 46,  10,   4,  19,  14,  26,  17,  19,   2,   6,   4,  17,  20,  14,\n",
            "          25,   4,  14,  10,  10,  14,  15,   4,  14,   6,   4,   2,   8,   4,\n",
            "          39,   1,  10,   7,  31,  19,  14,  27,  32, 100, 100, 100, 100, 100,\n",
            "         100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
            "         100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
            "         100, 100, 100, 100, 100, 100],\n",
            "        [ 33,  14,   4,   8,   1,   6,  17,   7,  19,  14,   4,  10,  11,  17,\n",
            "          19,  19,  49,   6,  17,  35,   4,  14,   6,   4,   3,  14,   4,  21,\n",
            "          17,   2,   3,   3,  17,   8,   6,   4,   9,   4,  25,  21,  17,  16,\n",
            "           2,  14,   4,  30,   1,   6,   4,   3,   2,  19,   4,  10,  17,   4,\n",
            "          26,   1,   7,   8,   6,  14,   4,  13,  14,   3,   4,   1,  19,   6,\n",
            "          14,   7,  10,   3,  37,  32]])\n",
            "back to text :  [\"-- J'ai été chez trois personnes... inutilement!\", 'Il reparut avec elle, et un Molière.', \"Le notaire l'arrêta; et se haussant à chaque mot sur la pointe des orteils:\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tv1Z1s-cQWiT"
      },
      "source": [
        "The model\n",
        "==\n",
        "For this model, we will not define a `forward` method, but two methods: `trainingLogits` and `predictionStrings`.\n",
        "\n",
        "*    `trainingLogits` is used at training time, when each batch is split in two parts: input paragraphs and output paragraphs. This function outputs, for each output paragraph of the batch, a log-probability distribution (i.e. a vector of \"logits\") before each token and after the last one. These distributions depend on the encoding of the corresponding input paragraph. They will then be used to compute a loss value.\n",
        "*    `predictionStrings` is used at prediction time, when each batch is only composed of input paragraphs. This function outputs, for each input paragraph, a string obtained by decoding the encoding of the paragraph.\n",
        "\n",
        "(Don't forget to read carefully all comments and to make sure that you understand them.)\n",
        "\n",
        "Here is a graphical representation of the architecture: https://moodle.u-paris.fr/mod/resource/view.php?id=648001\n",
        "Before starting the implementation, make sure you understand it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKfRCXQOOm8X"
      },
      "source": [
        "class Model(torch.nn.Module):\n",
        "  # 'size_vocabulary' does not include a padding character, but does include the end-of-paragraph one.\n",
        "  def __init__(self, size_vocabulary, EOP_id, embedding_dim, lstm_hidden_size, lstm_layers, device='cpu'):\n",
        "    super().__init__()\n",
        "\n",
        "    self.device = device\n",
        "\n",
        "    self.EOP_id = EOP_id # At prediction time, this index is used to stop the generation at the end of the paragraph.\n",
        "\n",
        "    # Here you have to define:\n",
        "    #  (i) an embedding layer 'self.char_embeddings' with 'torch.nn.Embedding' for the characters, including an padding embedding;\n",
        "    #  (ii) a bidirectional LSTM 'self.encoder_lstm' with a hidden size of 'lstm_hidden_size' and 'lstm_layers' layers (use batch_first=True);\n",
        "    #  (iii) a unidirectional LSTM 'self.decoder_lstm' with a hidden size of 'lstm_hidden_size' and 'lstm_layers' layers (use batch_first=True);\n",
        "    #  (iv) a network 'self.decoder_initialiser' meant to turn the final hidden and cell states of the encoder into the initial hidden and cell states of the decoder;\n",
        "    #  (v) a network 'self.distribution_nn' meant to turn the hidden state of the decoder at each step into the logits of a probability distribution over the vocabulary.\n",
        "        # The logits of a probability distribution are simply the log-probabilities (you might want to use torch.nn.LogSoftmax).\n",
        "    # Send all parts to 'device', so that we can use a GPU.\n",
        "    #################\n",
        "    self.size_vocabulary =size_vocabulary; self.embedding_dim = embedding_dim\n",
        "    self.lstm_hidden_size =lstm_hidden_size; self.lstm_layers =lstm_layers\n",
        "\n",
        "    self.char_embeddings = torch.nn.Embedding(self.size_vocabulary+1, self.embedding_dim, padding_idx = self.size_vocabulary).to(device)\n",
        "    self.encoder_lstm = torch.nn.LSTM(input_size = self.embedding_dim,\n",
        "                                      hidden_size = self.lstm_hidden_size,\n",
        "                                      ## Each LSTM cell has an interanl two vectors: the hidden state (h) and the cell state (c). The hidden_size refers to the dimensionality of these vectors.\n",
        "                                      num_layers = self.lstm_layers, bidirectional = True, batch_first=True).to(device)\n",
        "    self.decoder_lstm = torch.nn.LSTM(input_size = self.embedding_dim,\n",
        "                                      hidden_size = self.lstm_hidden_size,\n",
        "                                      num_layers = self.lstm_layers, bidirectional = False, batch_first=True).to(device) ## (bsize, seqlen, hidd)\n",
        "    self.decoder_initialiser = torch.nn.Linear(self.lstm_hidden_size * 2, self.lstm_hidden_size).to(device)\n",
        "\n",
        "    self.distribution_nn = torch.nn.Sequential(torch.nn.Linear(lstm_hidden_size, size_vocabulary),\n",
        "                                               torch.nn.LogSoftmax(dim=-1)).to(device)\n",
        "\n",
        "    # self.decoder_output = torch.nn.Linear(lstm_hidden_size, size_vocabulary).to(device)\n",
        "    # softmax = torch.nn.LogSoftmax(dim=-1).to(device)\n",
        "    # self.distribution_nn = softmax(self.decoder_output).to(device)\n",
        "\n",
        "\n",
        "    #################\n",
        "\n",
        "  # This function encodes the input paragraphs and turns them into initial states for the decoder. It is used both at training and prediction time.\n",
        "  # 'in_paragraphs' is a matrix (batch size, max in length) of character ids (Integer).\n",
        "  # You might want to understand what is the output of PyTorch's LSTMs: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "  def initStates(self, in_paragraphs):\n",
        "    batch_size = in_paragraphs.size(0)\n",
        "\n",
        "    in_char_embeddings = self.char_embeddings(in_paragraphs) # Shape: (batch_size, max length, embedding size)\n",
        "    #print(in_char_embeddings); print(in_char_embeddings.shape)\n",
        "    in_lengths = (in_paragraphs != self.char_embeddings.padding_idx).sum(axis=1) # Shape: (batch_size)\n",
        "    #print(in_lengths); print(in_lengths.shape)\n",
        "    in_char_embeddings = torch.nn.utils.rnn.pack_padded_sequence(input=in_char_embeddings, lengths=in_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "    # Enables the biLSTM to ignore padding elements. ## will obtain sequences concatenated in a single tensor\n",
        "\n",
        "    # The input paragraphs are encoded; the final hidden and cell states of the network will be used to initialise the decoder after a little transformation.\n",
        "    _, (h_n, c_n) = self.encoder_lstm(in_char_embeddings) # 'h_n' and 'c_n' are both of shape (num_layers * 2, batch_size, hidden_size)\n",
        "\n",
        "    # Concatenates the left-to-right and right-to-left final hidden states of the biLSTM.\n",
        "    h_n = h_n.view(self.encoder_lstm.num_layers, 2, batch_size, self.encoder_lstm.hidden_size)\n",
        "    # The second dimension (of size 2) of this tensor corresponds to left-to-right (0) and right-to-left (1).\n",
        "    #print(h_n); print(h_n.shape)\n",
        "    lr_h_n = h_n[:,0] # left-to-right; shape: (num_layers, batch_size, hidden_size)\n",
        "    rl_h_n = h_n[:,1] # right-to-left; shape: (num_layers, batch_size, hidden_size)\n",
        "    bi_h_n = torch.cat([lr_h_n, rl_h_n], axis=2) # Shape: (num_layers, batch_size, (2 * hidden_size))\n",
        "    #print(bi_h_n); print(bi_h_n.shape)\n",
        "\n",
        "    # Concatenates the left-to-right and right-to-left final cell states of the biLSTM.\n",
        "    c_n = c_n.view(self.encoder_lstm.num_layers, 2, batch_size, self.encoder_lstm.hidden_size)\n",
        "    # The second dimension (of size 2) of this tensor corresponds to left-to-right (0) and right-to-left (1).\n",
        "    #print(c_n); print(c_n.shape)\n",
        "    lr_c_n = c_n[:,0] # left-to-right; shape: (num_layers, batch_size, hidden_size)\n",
        "    rl_c_n = c_n[:,1] # right-to-left; shape: (num_layers, batch_size, hidden_size)\n",
        "    bi_c_n = torch.cat([lr_c_n, rl_c_n], axis=2) # Shape: (num_layers, batch_size, (2 * hidden_size))\n",
        "    #print(bi_c_n); print(bi_c_n.shape)\n",
        "\n",
        "    # What should be the shape of the two tensors of the following pair? Answer: (num_layers, batch_size, hidden_size)\n",
        "    return (self.decoder_initialiser(bi_h_n), self.decoder_initialiser(bi_c_n))\n",
        "\n",
        "  # Training time: This function outputs the logits for each time step.\n",
        "  # Because at training time, the output paragraph is known, there is no need to generate anything sequentially\n",
        "  #   — all positions can be processed at the same time. In fact, there is a loop hidden in the call to the decoder LSTM, but you should not write any explicit loop here.\n",
        "  # Do not forget the distribution for the first character.\n",
        "  # 'in_paragraphs' is a matrix (batch size, max in length) of character ids (Integer).\n",
        "  # 'out_paragraphs' is a matrix (batch size, max out length) of character ids (Integer) at training time. Assume it does not include the final end-of-paragraph character.\n",
        "  # You might want to understand what is the output of PyTorch's LSTMs: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "\n",
        "\n",
        "  def trainingLogits(self, in_paragraphs, out_paragraphs):\n",
        "    decoder_init_states = self.initStates(in_paragraphs)\n",
        "    # These tensors are not only used to initialise the decoder but also (for the first tensor) to compute the probability distributions for the first character.\n",
        "\n",
        "    # Feed a packed sequence to the decoder (use 'torch.nn.utils.rnn.pack_padded_sequence' and 'torch.nn.utils.rnn.pad_packed_sequence').\n",
        "    # You don't need to implement a loop, because at training time, you know in advance the decisions of the system (i.e. the tokens that are generated).\n",
        "    #################\n",
        "\n",
        "    out_char_embeddings = self.char_embeddings(out_paragraphs) # Shape: (batch_size, max length, embedding size)\n",
        "    #print(in_char_embeddings); print(in_char_embeddings.shape)\n",
        "    out_lengths = (out_paragraphs != self.char_embeddings.padding_idx).sum(axis=1) # Shape: (batch_size)\n",
        "    #print(in_lengths); print(in_lengths.shape)\n",
        "    out_char_embeddings = torch.nn.utils.rnn.pack_padded_sequence(input=out_char_embeddings, lengths=out_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "    # Enables the biLSTM to ignore padding elements. ## will obtain sequences are concatenated in a single tensor\n",
        "\n",
        "    # The input paragraphs are encoded; the final hidden and cell states of the network will be used to initialise the decoder after a little transformation.\n",
        "    '''EK done // double hash tag for my personal comment '''\n",
        "    ## here we want to concat (A)final hidden state of initializer & (B) ouput of decoder\n",
        "\n",
        "    ## (A)final hidden state of initializer\n",
        "    fin_decoder_init_states = decoder_init_states[0][-1][:][:] ## size (batch, hidden)\n",
        "    # print(fin_decoder_init_states.size())\n",
        "    ## (B)ouput of decoder\n",
        "    packed_out, (_, _) = self.decoder_lstm(out_char_embeddings, decoder_init_states) ## size of packed_output : vocab size?\n",
        "    unpacked_out, unpacked_len = torch.nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True) ## size of unpacked_out : ( batch X max_seq_len X hidden_dim)\n",
        "    # decoder_out = self.decoder_output(unpacked_out) # shape = (batch_size, max length, hidden size)\n",
        "    # print(\"unpacked_out\",unpacked_out.size())\n",
        "    ## concat of A&B is  (batch, hidden) & (batch, max_seq_len, hidden); to concatenate, 2 dim to be matched which is batch and hid\n",
        "    ## so lets make   (1, batch, hidden) & (max_seq_len, batch, hidden)\n",
        "    fin_decoder_init_states = torch.unsqueeze(fin_decoder_init_states, 0) ## (1, batch, hidden)\n",
        "    # print(\"fin_decoder_init_states - \", fin_decoder_init_states.size())\n",
        "\n",
        "    permuted_out = torch.permute(unpacked_out,(1,0,2)) ## (max_seq_len, batch, hidden)\n",
        "    # print(\"permuted_out - \", permuted_out.size())\n",
        "\n",
        "    concatenated = torch.cat((fin_decoder_init_states, permuted_out)) ## (1+max_seq_len, batch, hidden)\n",
        "    concatenated = torch.permute(concatenated,(1,0,2)) ## (B, 1+max_seq_len, hidden)\n",
        "    final_proba = self.distribution_nn(concatenated) ## (B, 1+max_seq_len, size_voca)\n",
        "\n",
        "    # print(\"final_proba - \", final_proba.size())\n",
        "    return final_proba[:, :-1, :]\n",
        "\n",
        "    # permuted = torch.permute(decoder_init_states[0],(1,0,2))[:][-1][:]\n",
        "    # print(permuted.size())\n",
        "    # print(unpacked_out.size())\n",
        "    # concatenated = torch.cat((permuted, unpacked_out))\n",
        "\n",
        "\n",
        "    #################\n",
        "\n",
        "  # Prediction time: This function generates a text up to 'max_predicted_char' character long for each paragraph in the batch.\n",
        "  # 'in_paragraphs' is a matrix (batch size, max in length) of character ids (Integer).\n",
        "  # You might want to understand what is the output of PyTorch's LSTMs: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
        "  def predictionStrings(self, in_paragraphs, max_predicted_char=1000):\n",
        "    batch_size = in_paragraphs.size(0)\n",
        "\n",
        "    decoder_init_states = self.initStates(in_paragraphs) # These tensors are not only used to initialise the decoder but also (for the first tensor) to compute the probability distributions for the first character.\n",
        "\n",
        "    # Decode 'decoder_init_states' into a matrix a character ids (on line per input paragraph in the batch) and then convert it to strings of actual characters.\n",
        "    # You will need to implement a loop at some point.\n",
        "    # To work with probability distributions, you may use \"torch.distributions.Categorical\", but not necessarily.\n",
        "    #################\n",
        "\n",
        "    hidden, cell = decoder_init_states\n",
        "    final_hidden = hidden[-1][:][:] # (batch, hidden)\n",
        "    final_hidden = final_hidden.unsqueeze(0) # (seq_len, batch, hidden)\n",
        "    final_hidden = final_hidden.permute(1,0,2) # (batch, seq_len, hidden)\n",
        "\n",
        "\n",
        "\n",
        "    proba_first_char = self.distribution_nn(final_hidden)  # (batch, seq_len, voca_size)\n",
        "    first_char_id = torch.argmax(proba_first_char, dim =2) #(batch, seq_len)\n",
        "\n",
        "    seq_ids = torch.clone(first_char_id) #(batch, seq_len)\n",
        "\n",
        "    char_embedding = self.char_embeddings(first_char_id) #(batch, seq_len, emb_dim)\n",
        "\n",
        "    for char in range(max_predicted_char):\n",
        "     out, (hidden,cell) = self.decoder_lstm(char_embedding, (hidden, cell))\n",
        "     proba_char = self.distribution_nn(out)\n",
        "     char_id = torch.argmax(proba_char, dim =2)\n",
        "    #  char_id = torch.distributions.Categorical(proba_char).sample()\n",
        "    #  print(char_id)\n",
        "    #  print(char_id[-1])\n",
        "     seq_ids = torch.cat((seq_ids, char_id), 1)\n",
        "     char_embedding = self.char_embeddings(char_id) #(batch, seq_len, emb_dim)\n",
        "\n",
        "    texts = ids_to_texts(seq_ids)\n",
        "    return texts\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5TXsvizgogZ"
      },
      "source": [
        "model = Model(size_vocabulary=len(char_vocabulary), EOP_id=EOP_id, embedding_dim=19, lstm_hidden_size=13, lstm_layers=7, device='cuda')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tests the training method (again).\n",
        "in_paragraphs = torch.tensor([(list(range(5)) + ([batch_generator.padding_idx] * 10)), (list(range(10)) + ([batch_generator.padding_idx] * 5))]).to(model.device)\n",
        "# A batch that contains two sentences with some padding (more than necessary).\n",
        "\n",
        "print(in_paragraphs)\n",
        "out_paragraphs = in_paragraphs\n",
        "model.trainingLogits(in_paragraphs, out_paragraphs)"
      ],
      "metadata": {
        "id": "20LbQYTgTClD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4a61016-f72c-4e58-bc37-4dad23c9934a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  0,   1,   2,   3,   4, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
            "         100],\n",
            "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9, 100, 100, 100, 100,\n",
            "         100]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-4.2358, -4.5068, -4.8697,  ..., -4.6520, -4.6366, -4.2372],\n",
              "         [-4.3794, -4.5103, -4.8153,  ..., -4.6417, -4.6069, -4.4068],\n",
              "         [-4.4052, -4.5052, -4.7866,  ..., -4.6241, -4.6057, -4.4560],\n",
              "         ...,\n",
              "         [-4.3816, -4.5838, -4.7885,  ..., -4.6088, -4.6250, -4.4122],\n",
              "         [-4.3816, -4.5838, -4.7885,  ..., -4.6088, -4.6250, -4.4122],\n",
              "         [-4.3816, -4.5838, -4.7885,  ..., -4.6088, -4.6250, -4.4122]],\n",
              "\n",
              "        [[-4.2405, -4.5060, -4.8683,  ..., -4.6538, -4.6359, -4.2351],\n",
              "         [-4.3819, -4.5099, -4.8145,  ..., -4.6428, -4.6066, -4.4056],\n",
              "         [-4.4066, -4.5049, -4.7861,  ..., -4.6247, -4.6057, -4.4553],\n",
              "         ...,\n",
              "         [-4.4190, -4.4963, -4.7568,  ..., -4.5926, -4.6260, -4.4984],\n",
              "         [-4.4181, -4.4961, -4.7563,  ..., -4.5908, -4.6278, -4.4993],\n",
              "         [-4.4174, -4.4961, -4.7560,  ..., -4.5896, -4.6290, -4.4998]]],\n",
              "       device='cuda:0', grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4sp6c9Bdch3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a76b1c44-0777-4d94-a47b-375a26dca52f"
      },
      "source": [
        "# Tests the training method.\n",
        "in_paragraphs = torch.tensor([(list(range(5)) + ([batch_generator.padding_idx] * 0))]).to(model.device)\n",
        "# A batch that contains only one sentence with no padding.\n",
        "print(in_paragraphs)\n",
        "out_paragraphs = in_paragraphs\n",
        "model.trainingLogits(in_paragraphs, out_paragraphs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0, 1, 2, 3, 4]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-4.2358, -4.5068, -4.8697, -4.8595, -4.5959, -4.6761, -4.3968,\n",
              "          -5.0175, -4.4828, -4.7382, -4.4449, -4.6436, -4.8952, -4.3654,\n",
              "          -4.3695, -4.4616, -4.5954, -4.4299, -4.5288, -5.0503, -4.7347,\n",
              "          -4.7167, -4.4404, -4.5566, -4.5392, -4.4191, -4.7795, -4.5659,\n",
              "          -4.7698, -4.8030, -4.6554, -4.9666, -4.5925, -4.4994, -4.7906,\n",
              "          -4.3801, -4.3415, -4.2707, -4.8580, -4.4736, -4.4353, -4.6041,\n",
              "          -4.9328, -4.7458, -4.8218, -4.5234, -4.6623, -4.3564, -4.5188,\n",
              "          -4.7601, -4.9271, -4.5766, -4.5568, -4.5569, -4.6334, -4.7228,\n",
              "          -4.6722, -4.2496, -4.7030, -4.3837, -4.6527, -4.6390, -4.6827,\n",
              "          -4.6894, -4.5634, -4.4429, -4.5277, -4.3899, -4.6573, -4.9238,\n",
              "          -4.9086, -4.6949, -4.4053, -4.6746, -4.8806, -4.6918, -4.8593,\n",
              "          -4.3735, -4.4694, -4.6711, -4.4048, -4.8286, -4.7878, -4.6702,\n",
              "          -4.4909, -4.4820, -4.6091, -4.7716, -4.8147, -4.4582, -4.6196,\n",
              "          -4.7002, -4.8647, -4.7711, -4.5218, -4.6622, -4.7637, -4.6520,\n",
              "          -4.6366, -4.2372],\n",
              "         [-4.3794, -4.5103, -4.8153, -4.8957, -4.6058, -4.6627, -4.4765,\n",
              "          -4.9527, -4.5997, -4.8432, -4.3886, -4.5521, -4.8822, -4.3953,\n",
              "          -4.3803, -4.5631, -4.3984, -4.6000, -4.5421, -4.9924, -4.7294,\n",
              "          -4.8264, -4.5317, -4.5701, -4.5569, -4.4621, -4.8989, -4.6230,\n",
              "          -4.8616, -4.7577, -4.6799, -4.8471, -4.5625, -4.5475, -4.8882,\n",
              "          -4.3952, -4.3749, -4.3800, -4.8561, -4.4828, -4.4502, -4.4722,\n",
              "          -4.8020, -4.7832, -4.6666, -4.5763, -4.6758, -4.3643, -4.4468,\n",
              "          -4.7109, -4.8821, -4.6683, -4.4950, -4.4613, -4.6306, -4.7933,\n",
              "          -4.7051, -4.3585, -4.7589, -4.4465, -4.7124, -4.5565, -4.6651,\n",
              "          -4.5752, -4.4756, -4.3636, -4.6350, -4.4106, -4.5837, -4.8812,\n",
              "          -4.7165, -4.6523, -4.4745, -4.4888, -4.8943, -4.7363, -4.8595,\n",
              "          -4.4622, -4.4483, -4.4384, -4.3958, -4.7397, -4.7432, -4.5700,\n",
              "          -4.4431, -4.4052, -4.7603, -4.7869, -4.8574, -4.4911, -4.5714,\n",
              "          -4.7496, -4.8517, -4.8700, -4.4123, -4.5104, -4.7034, -4.6417,\n",
              "          -4.6069, -4.4068],\n",
              "         [-4.4052, -4.5052, -4.7866, -4.9005, -4.6129, -4.6449, -4.5023,\n",
              "          -4.9352, -4.6423, -4.8697, -4.3617, -4.5243, -4.8918, -4.4176,\n",
              "          -4.3874, -4.6055, -4.3404, -4.6422, -4.5328, -4.9674, -4.7190,\n",
              "          -4.8435, -4.5614, -4.5678, -4.5523, -4.4698, -4.9227, -4.6336,\n",
              "          -4.8927, -4.7660, -4.6969, -4.8025, -4.5493, -4.5678, -4.9202,\n",
              "          -4.3926, -4.3893, -4.4093, -4.8448, -4.4857, -4.4701, -4.4450,\n",
              "          -4.7480, -4.7881, -4.6410, -4.6185, -4.6662, -4.3473, -4.4389,\n",
              "          -4.7047, -4.8557, -4.7013, -4.4862, -4.4166, -4.6547, -4.8210,\n",
              "          -4.7341, -4.3940, -4.7728, -4.4634, -4.7146, -4.5343, -4.6758,\n",
              "          -4.5044, -4.4540, -4.3467, -4.6615, -4.4206, -4.5730, -4.8917,\n",
              "          -4.6822, -4.6267, -4.4936, -4.4648, -4.8963, -4.7547, -4.8701,\n",
              "          -4.4786, -4.4741, -4.3895, -4.3900, -4.7133, -4.7277, -4.5431,\n",
              "          -4.4382, -4.4175, -4.8109, -4.7963, -4.8698, -4.5210, -4.5442,\n",
              "          -4.7510, -4.8504, -4.8842, -4.3790, -4.4585, -4.6855, -4.6241,\n",
              "          -4.6057, -4.4560],\n",
              "         [-4.4155, -4.5017, -4.7718, -4.8985, -4.6156, -4.6313, -4.5143,\n",
              "          -4.9225, -4.6686, -4.8831, -4.3482, -4.5153, -4.9002, -4.4339,\n",
              "          -4.3902, -4.6300, -4.3096, -4.6628, -4.5254, -4.9487, -4.7132,\n",
              "          -4.8509, -4.5793, -4.5690, -4.5466, -4.4749, -4.9333, -4.6384,\n",
              "          -4.9051, -4.7769, -4.7095, -4.7784, -4.5416, -4.5762, -4.9383,\n",
              "          -4.3884, -4.3987, -4.4254, -4.8372, -4.4876, -4.4825, -4.4343,\n",
              "          -4.7138, -4.7851, -4.6306, -4.6452, -4.6618, -4.3386, -4.4328,\n",
              "          -4.7027, -4.8436, -4.7252, -4.4856, -4.3913, -4.6735, -4.8386,\n",
              "          -4.7554, -4.4122, -4.7797, -4.4704, -4.7116, -4.5273, -4.6844,\n",
              "          -4.4569, -4.4435, -4.3370, -4.6766, -4.4277, -4.5702, -4.8982,\n",
              "          -4.6665, -4.6082, -4.5053, -4.4607, -4.8926, -4.7674, -4.8740,\n",
              "          -4.4811, -4.4909, -4.3641, -4.3883, -4.7008, -4.7165, -4.5293,\n",
              "          -4.4378, -4.4306, -4.8394, -4.8040, -4.8801, -4.5422, -4.5321,\n",
              "          -4.7464, -4.8455, -4.8897, -4.3555, -4.4339, -4.6718, -4.6124,\n",
              "          -4.6100, -4.4785],\n",
              "         [-4.4192, -4.4993, -4.7640, -4.8941, -4.6157, -4.6219, -4.5199,\n",
              "          -4.9133, -4.6849, -4.8900, -4.3420, -4.5139, -4.9063, -4.4455,\n",
              "          -4.3908, -4.6437, -4.2923, -4.6723, -4.5198, -4.9362, -4.7106,\n",
              "          -4.8543, -4.5902, -4.5714, -4.5422, -4.4784, -4.9378, -4.6404,\n",
              "          -4.9092, -4.7868, -4.7180, -4.7653, -4.5368, -4.5794, -4.9483,\n",
              "          -4.3844, -4.4040, -4.4346, -4.8324, -4.4887, -4.4902, -4.4306,\n",
              "          -4.6927, -4.7816, -4.6263, -4.6617, -4.6604, -4.3339, -4.4277,\n",
              "          -4.7025, -4.8384, -4.7412, -4.4878, -4.3764, -4.6871, -4.8497,\n",
              "          -4.7701, -4.4218, -4.7829, -4.4732, -4.7074, -4.5260, -4.6910,\n",
              "          -4.4260, -4.4386, -4.3306, -4.6860, -4.4330, -4.5700, -4.9019,\n",
              "          -4.6595, -4.5955, -4.5123, -4.4626, -4.8882, -4.7761, -4.8746,\n",
              "          -4.4792, -4.5013, -4.3501, -4.3880, -4.6949, -4.7089, -4.5215,\n",
              "          -4.4391, -4.4407, -4.8558, -4.8102, -4.8880, -4.5567, -4.5270,\n",
              "          -4.7413, -4.8402, -4.8910, -4.3391, -4.4222, -4.6618, -4.6044,\n",
              "          -4.6153, -4.4891]]], device='cuda:0', grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tests the training method (again).\n",
        "in_paragraphs = torch.tensor([(list(range(5)) + ([batch_generator.padding_idx] * 10)), (list(range(10)) + ([batch_generator.padding_idx] * 5))]).to(model.device)\n",
        "# A batch that contains two sentences with some padding (more than necessary).\n",
        "print(in_paragraphs)\n",
        "out_paragraphs = in_paragraphs\n",
        "model.trainingLogits(in_paragraphs, out_paragraphs)"
      ],
      "metadata": {
        "id": "ftAttD70si-O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26025480-8aef-4f96-97a4-55f640c4ddf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[  0,   1,   2,   3,   4, 100, 100, 100, 100, 100, 100, 100, 100, 100,\n",
            "         100],\n",
            "        [  0,   1,   2,   3,   4,   5,   6,   7,   8,   9, 100, 100, 100, 100,\n",
            "         100]], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-4.2358, -4.5068, -4.8697,  ..., -4.6520, -4.6366, -4.2372],\n",
              "         [-4.3794, -4.5103, -4.8153,  ..., -4.6417, -4.6069, -4.4068],\n",
              "         [-4.4052, -4.5052, -4.7866,  ..., -4.6241, -4.6057, -4.4560],\n",
              "         ...,\n",
              "         [-4.3816, -4.5838, -4.7885,  ..., -4.6088, -4.6250, -4.4122],\n",
              "         [-4.3816, -4.5838, -4.7885,  ..., -4.6088, -4.6250, -4.4122],\n",
              "         [-4.3816, -4.5838, -4.7885,  ..., -4.6088, -4.6250, -4.4122]],\n",
              "\n",
              "        [[-4.2405, -4.5060, -4.8683,  ..., -4.6538, -4.6359, -4.2351],\n",
              "         [-4.3819, -4.5099, -4.8145,  ..., -4.6428, -4.6066, -4.4056],\n",
              "         [-4.4066, -4.5049, -4.7861,  ..., -4.6247, -4.6057, -4.4553],\n",
              "         ...,\n",
              "         [-4.4190, -4.4963, -4.7568,  ..., -4.5926, -4.6260, -4.4984],\n",
              "         [-4.4181, -4.4961, -4.7563,  ..., -4.5908, -4.6278, -4.4993],\n",
              "         [-4.4174, -4.4961, -4.7560,  ..., -4.5896, -4.6290, -4.4998]]],\n",
              "       device='cuda:0', grad_fn=<SliceBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## split\n",
        "# in_paragraphs = train[0]\n",
        "# print(in_paragraphs)\n",
        "# out_paragraphs = train[1]\n",
        "# model.trainingLogits(in_paragraphs, out_paragraphs)"
      ],
      "metadata": {
        "id": "A-_FEUHsW_s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRPGm_Duq_QY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb54844-d7ee-439c-b0c9-ed1a6631fa88"
      },
      "source": [
        "# Tests the prediction methods.\n",
        "batch = batch_generator.get_batch(2)\n",
        "model.predictionStrings(batch[0].to(model.device), max_predicted_char=16)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ºDqqqqqqqqqqqqqqq', 'ºDqqqqqqqqqqqqqqq']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "OYDPYRxmeBsj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df657486-a23e-44c0-c52a-164405f7d2de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def get_bleu(reference, candidate, smoothing_function=SmoothingFunction().method1):\n",
        "  # Tokenizing the reference and candidate texts\n",
        "  reference_tok = [word_tokenize(reference[0])]\n",
        "  candidate_tok = word_tokenize( candidate[0])\n",
        "  return sentence_bleu(reference_tok, candidate_tok)\n"
      ],
      "metadata": {
        "id": "QaHQhipwp7FR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80KIRPPyOCWQ"
      },
      "source": [
        "Training\n",
        "=="
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdJSBtNGCX-J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7d055255-a8a6-4790-d546-870ec1d59261"
      },
      "source": [
        "##grid search\n",
        "for learning_rate in [0.02, 0.2]:\n",
        "  for embedding_dim in [128,256,512]:\n",
        "    for lstm_hidden_size in [256, 512, 1024]:\n",
        "      for lstm_layers in [1,2,3]:\n",
        "        for batch_size in [32, 64]:\n",
        "          print(\"lr,emb,hid,lay,B : \", (learning_rate, embedding_dim, lstm_hidden_size , lstm_layers, batch_size))\n",
        "\n",
        "          # model = Model(size_vocabulary=len(char_vocabulary), EOP_id=EOP_id, embedding_dim = 256, lstm_hidden_size=512, lstm_layers=1, device='cuda')\n",
        "          model = Model(size_vocabulary=len(char_vocabulary), EOP_id=EOP_id, embedding_dim = embedding_dim, lstm_hidden_size=lstm_hidden_size, lstm_layers=lstm_layers, device='cuda')\n",
        "\n",
        "          import time\n",
        "\n",
        "          model.eval() # Tells Pytorch we are in evaluation/inference mode (can be useful if dropout is used, for instance).\n",
        "\n",
        "          # Training procedure\n",
        "          # learning_rate = 0.2\n",
        "          learning_rate = learning_rate\n",
        "          momentum = 0.99\n",
        "          l2_reg = 0.0001\n",
        "          optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=l2_reg)\n",
        "          # Once the backward propagation has been done, call the 'step' method (with no argument) to update the parameters.\n",
        "          # batch_size = 64 if(not use_toy_dataset) else 128\n",
        "          batch_size = batch_size if(not use_toy_dataset) else 128\n",
        "          subset = None # Use an integer to train on a smaller portion of the training set, otherwise use None.\n",
        "          epoch_size = batch_generator.length() if(subset is None) else subset # In number of instances\n",
        "\n",
        "          nb_epoch = 20 if(not use_toy_dataset) else 20\n",
        "          epoch_id = 0 # Id of the current epoch\n",
        "          instances_processed = 0 # Number of instances trained on in the current epoch\n",
        "          epoch_loss = [] # Will contain the loss for each batch of the current epoch\n",
        "          time_0 = time.time()\n",
        "          while(epoch_id < nb_epoch):\n",
        "            model.train() # Tells Pytorch we are in training mode (can be useful if dropout is used, for instance).\n",
        "\n",
        "            model.zero_grad() # Makes sure the gradient is reinitialised to zero.\n",
        "\n",
        "            batch = batch_generator.get_batch(batch_size, subset=subset)\n",
        "\n",
        "            #print(ids_to_texts(batch[0])); print(ids_to_texts(batch[1]))\n",
        "            in_paragraphs = batch[0].to(model.device)\n",
        "            out_paragraphs = batch[1].to(model.device)\n",
        "\n",
        "            # You have to (i) compute the prediction of the model, (ii) compute the loss, (iii) call \"backward\" on the loss and (iv) store the loss in \"epoch_loss\".\n",
        "            # For the loss, use torch.nn.functional.nll_loss. Computes an average over all tokens of the batch, but do not take into account distribution logits that corresonds to padding characters. Read the documentation and be careful about the shape of your tensors.\n",
        "            ###################\n",
        "\n",
        "            logits = model.trainingLogits(in_paragraphs, out_paragraphs)\n",
        "            # print(logits.shape)\n",
        "\n",
        "            logits = logits.transpose(1, 2)\n",
        "\n",
        "            pad_idx = batch_generator.padding_idx\n",
        "\n",
        "            loss = torch.nn.functional.nll_loss(logits, out_paragraphs, reduction=\"mean\", ignore_index=pad_idx)\n",
        "\n",
        "            loss.backward()\n",
        "\n",
        "            epoch_loss.append(loss.item())\n",
        "\n",
        "            ###################\n",
        "\n",
        "            optimizer.step() # Updates the parameters.\n",
        "\n",
        "            instances_processed += batch_size\n",
        "            if(instances_processed > epoch_size):\n",
        "              print(f\"-- END OF EPOCH {epoch_id}.\")\n",
        "              print(f\"Average loss: {sum(epoch_loss) / len(epoch_loss)}.\")\n",
        "              duration = time.time() - time_0\n",
        "              print(f\"{duration} s elapsed (i.e. {duration / (epoch_id + 1)} s/epoch)\")\n",
        "\n",
        "              # Example of generation\n",
        "              batch = batch_generator.get_batch(1, subset=subset)\n",
        "              print(ids_to_texts(batch[0])) # Input paragraph\n",
        "              generated = model.predictionStrings(batch[0].to(model.device), max_predicted_char=512) # Generated output paragraph.\n",
        "              print(generated)\n",
        "\n",
        "              ## BLEU\n",
        "              print(f\"BLEU : {round(get_bleu(ids_to_texts(batch[1]), generated),3)}\")\n",
        "\n",
        "              epoch_id += 1\n",
        "              instances_processed -= epoch_size\n",
        "              epoch_loss = []"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "lr,emb,hid,lay,B :  (0.02, 128, 256, 1, 32)\n",
            "-- END OF EPOCH 0.\n",
            "Average loss: 2.921983603898644.\n",
            "27.10006546974182 s elapsed (i.e. 27.10006546974182 s/epoch)\n",
            "[\"La mère Bovary, les jours suivants, fut très étonnée de la métamorphose de sa bru. En effet, Emma se montra plus docile, et même poussa la déférence jusqu'à lui demander une recette pour faire mariner des cornichons.\"]\n",
            "['---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 1.\n",
            "Average loss: 2.191952940152616.\n",
            "49.78644347190857 s elapsed (i.e. 24.893221735954285 s/epoch)\n",
            "[\"À une députation de Bordelais:--Ce qui me console de n'être pas à Bordeaux c'est de me trouver au milieu de vous!\"]\n",
            "['Elle de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de parte de de p']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU : 0.0\n",
            "-- END OF EPOCH 2.\n",
            "Average loss: 1.9717096096398878.\n",
            "72.34357953071594 s elapsed (i.e. 24.114526510238647 s/epoch)\n",
            "['-- Mais il y a peut-être à Rouen, dit le médecin, plusieurs demoiselles Lempereur qui sont maîtresses de piano?']\n",
            "['-- Elle se partion de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 3.\n",
            "Average loss: 1.8551520474103032.\n",
            "94.49635791778564 s elapsed (i.e. 23.62408947944641 s/epoch)\n",
            "[\"Il allait chez eux, pour leur faire part d'un malheur. Le vent, la nuit dernière, avait jeté bas vingt pommiers dans les cours, abattu la bouillerie, enlevé le toit de la grange. Ils passèrent le reste de l'après-midi à constater les dégâts, et le lendemain, avec le charpentier, le maçon, et le couvreur. Les réparations monteraient à dix-huit cents francs, pour le moins.\"]\n",
            "['-- Ces par le partion de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 4.\n",
            "Average loss: 1.7712301599736116.\n",
            "116.65206861495972 s elapsed (i.e. 23.330413722991942 s/epoch)\n",
            "['Donc les règles ne suffisent pas. Il faut, de plus, le génie.']\n",
            "[\"-- C'est les partes de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 5.\n",
            "Average loss: 1.7088411450386047.\n",
            "140.08827900886536 s elapsed (i.e. 23.348046501477558 s/epoch)\n",
            "[\"Au milieu du silence qui emplissait le village, un cri déchirant traversa l'air. Bovary devint pâle à s'évanouir. Elle fronça les sourcils d'un geste nerveux, puis continua. C'était pour lui cependant, pour cet être, pour cet homme qui ne comprenait rien, qui ne sentait rien! car il était là, tout tranquillement, et sans même se douter que le ridicule de son nom allait désormais la salir comme lui. Elle avait fait des efforts pour l'aimer, et elle s'était repentie en pleurant d'avoir cédé à un autre.\"]\n",
            "['-- Charles de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la co']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 6.\n",
            "Average loss: 1.6547993312632372.\n",
            "161.96108150482178 s elapsed (i.e. 23.13729735783168 s/epoch)\n",
            "['--Pourquoi, bêtises?']\n",
            "['-- Ah! comme de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couvert de la couv']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 7.\n",
            "Average loss: 1.6048652432402786.\n",
            "185.9548795223236 s elapsed (i.e. 23.24435994029045 s/epoch)\n",
            "[\"Quand ils furent devant la maison du maréchal, au lieu de suivre la route jusqu'à la barrière, Rodolphe, brusquement, prit un sentier, entraînant madame Bovary; il cria:\"]\n",
            "['-- Elle se rentre des proches de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 8.\n",
            "Average loss: 1.5650773857321059.\n",
            "210.30500531196594 s elapsed (i.e. 23.36722281244066 s/epoch)\n",
            "[\"Elle retira Berthe de nourrice. Félicité l'amenait quand il venait des visites, et madame Bovary la déshabillait afin de faire voir ses membres. Elle déclarait adorer les enfants; c'était sa consolation, sa joie, sa folie, et elle accompagnait ses caresses d'expansions lyriques, qui, à d'autres qu'à des Yonvillais, eussent rappelé la Sachette de Notre-Dame de Paris.\"]\n",
            "['-- Ah! mais le promaise de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 9.\n",
            "Average loss: 1.5273440346425893.\n",
            "232.0721344947815 s elapsed (i.e. 23.20721344947815 s/epoch)\n",
            "['Elle sanglotait.']\n",
            "['-- Ah! mais le contre les contraires de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 10.\n",
            "Average loss: 1.4991414601705513.\n",
            "254.9056191444397 s elapsed (i.e. 23.17323810403997 s/epoch)\n",
            "[\"I° S'être, lors du choléra, signalé par un dévouement sans bornes; 2° avoir publié, et à mes frais, différents ouvrages d'utilité publique, tels que... (et il rappelait son mémoire intitulé: Du cidre, de sa fabrication et de ses effets; plus, des observations sur le puceron laniger, envoyées à l'Académie; son volume de statistique, et jusqu'à sa thèse de pharmacien); sans compter que je suis membre de plusieurs sociétés savantes (il l'était d'une seule).\"]\n",
            "['-- Ah! mais le conseille de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de la changeaient de']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 11.\n",
            "Average loss: 1.4711396201532714.\n",
            "278.4684987068176 s elapsed (i.e. 23.205708225568134 s/epoch)\n",
            "[\"Juillet 1830. Marmont lui apprend l'état des affaires. Alors il entre dans une telle fureur qu'il se blesse la main à l'épée du général.\"]\n",
            "[\"-- Ce n'est pas de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la cheveux de la c\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 12.\n",
            "Average loss: 1.447776245586763.\n",
            "301.5473656654358 s elapsed (i.e. 23.195951205033523 s/epoch)\n",
            "['-- Pleurez, reprit le pharmacien, donnez cours à la nature, cela vous soulagera!']\n",
            "['-- Comment le continuait le continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la continuelle de la']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 13.\n",
            "Average loss: 1.4317053769315993.\n",
            "324.10429310798645 s elapsed (i.e. 23.15030665057046 s/epoch)\n",
            "[\"Tous les fabriciens se regardèrent, fort ébahis, et comme pour s'assurer qu'ils n'étaient pas des singes.\"]\n",
            "[\"-- Ce n'est pas de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la couleur de la c\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 14.\n",
            "Average loss: 1.41209726552574.\n",
            "347.3996410369873 s elapsed (i.e. 23.159976069132487 s/epoch)\n",
            "[\"-- Oui..., c'est vrai..., tu es bon, toi!\"]\n",
            "['-- Ah! comme un charrant de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambr']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 15.\n",
            "Average loss: 1.396134417884204.\n",
            "370.5363302230835 s elapsed (i.e. 23.15852063894272 s/epoch)\n",
            "['Bouvard les reçut, et commença la démonstration par le vestibule.']\n",
            "['-- Ah! comme les considérations de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération d']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 16.\n",
            "Average loss: 1.3797749408653803.\n",
            "393.27034878730774 s elapsed (i.e. 23.13354992866516 s/epoch)\n",
            "[\"Charles se récria encore une fois qu'il ne pouvait s'absenter plus longtemps; mais rien n'empêchait Emma...\"]\n",
            "['-- Eh mon autre de son coup de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la constitude de la con']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 17.\n",
            "Average loss: 1.3723681952272142.\n",
            "416.05752515792847 s elapsed (i.e. 23.114306953218247 s/epoch)\n",
            "[\"--C'est mon parrain répliqua Bouvard, négligemment, ajoutant qu'il s'appelait de ses noms de baptême François, Denys, Bartholomée. Ceux de Pécuchet étaient Juste, Romain, Cyrille;--et ils avaient le même âge: quarante-sept ans! Cette coïncidence leur fit plaisir; mais les surprit, chacun ayant cru l'autre beaucoup moins jeune. Ensuite, ils admirèrent la Providence dont les combinaisons parfois sont merveilleuses.--Car, enfin, si nous n'étions pas sortis tantôt pour nous promener, nous aurions pu mourir avant de nous connaître! et s'étant donné l'adresse de leurs patrons, ils se souhaitèrent une bonne nuit.\"]\n",
            "['-- Et il avait pas de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire de la conduire']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 18.\n",
            "Average loss: 1.3619278353087756.\n",
            "439.73070073127747 s elapsed (i.e. 23.14372109111987 s/epoch)\n",
            "[\"Charles, cependant, alla prier un domestique d'atteler son boc. On l'amena devant le perron, et, tous les paquets y étant fourrés, les époux Bovary firent leurs politesses au Marquis et à la Marquise, et repartirent pour Tostes.\"]\n",
            "['-- Ce qui se renversait des chambres de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 19.\n",
            "Average loss: 1.3470182836358318.\n",
            "461.07046270370483 s elapsed (i.e. 23.05352313518524 s/epoch)\n",
            "[\"Elle se tenait en face, appuyée contre la cloison de la chaloupe, où la lune entrait par un des volets ouverts. Sa robe noire, dont les draperies s'élargissaient en éventail, l'amincissait, la rendait plus grande. Elle avait la tête levée, les mains jointes, et les deux yeux vers le ciel. Parfois l'ombre des saules la cachait en entier, puis elle réapparaissait tout à coup, comme une vision, dans la lumière de la lune.\"]\n",
            "['-- Comment en pas de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour ']\n",
            "BLEU : 0.0\n",
            "lr,emb,hid,lay,B :  (0.02, 128, 256, 1, 64)\n",
            "-- END OF EPOCH 0.\n",
            "Average loss: 3.323374779537471.\n",
            "19.94177484512329 s elapsed (i.e. 19.94177484512329 s/epoch)\n",
            "[\"--Comme l'agronomie! répliqua Bouvard.\"]\n",
            "[' lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait lait la']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 1.\n",
            "Average loss: 2.496790066057322.\n",
            "38.25829553604126 s elapsed (i.e. 19.12914776802063 s/epoch)\n",
            "[\"--Laisse-moi partir avec toi! je serai ta domestique! Tu as besoin de quelqu'un. Mais ne t'en va pas! ne me quitte pas! La mort plutôt! Tue-moi!\"]\n",
            "['---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 2.\n",
            "Average loss: 2.260802534161782.\n",
            "58.7755184173584 s elapsed (i.e. 19.5918394724528 s/epoch)\n",
            "[\"Elle laissait maintenant tout aller dans son ménage, et madame Bovary mère, lorsqu'elle vint passer à Tostes une partie du carême, s'étonna fort de ce changement. Elle, en effet, si soigneuse autrefois et délicate, elle restait à présent des journées entières sans s'habiller, portait des bas de coton gris, s'éclairait à la chandelle. Elle répétait qu'il fallait économiser, puisqu'ils n'étaient pas riches, ajoutant qu'elle était très contente, très heureuse, que Tostes lui plaisait beaucoup, et autres discours nouveaux qui fermaient la bouche à la belle-mère. Du reste, Emma ne semblait plus disposée à suivre ses conseils; une fois même, madame Bovary s'étant avisée de prétendre que les maîtres devaient surveiller la religion de leurs domestiques, elle lui avait répondu d'un oeil si colère et avec un sourire tellement froid, que la bonne femme ne s'y frotta plus.\"]\n",
            "['Le le de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 3.\n",
            "Average loss: 2.114627385626034.\n",
            "78.79894828796387 s elapsed (i.e. 19.699737071990967 s/epoch)\n",
            "[\"Ils ne doutèrent plus d'eux-mêmes, et appelant les deux élèves recommencèrent l'analyse de leur boîte osseuse.\"]\n",
            "['Il se conte de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de comme de com']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 4.\n",
            "Average loss: 2.0072885934187443.\n",
            "98.86025810241699 s elapsed (i.e. 19.7720516204834 s/epoch)\n",
            "['-- Mal, répondit Emma; je souffre.']\n",
            "['-- Le la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 5.\n",
            "Average loss: 1.933296459061759.\n",
            "118.32582783699036 s elapsed (i.e. 19.720971306165058 s/epoch)\n",
            "['-- Bon voyage!']\n",
            "['-- Elle se comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de l']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 6.\n",
            "Average loss: 1.876426565403841.\n",
            "138.35114312171936 s elapsed (i.e. 19.76444901738848 s/epoch)\n",
            "['--Nullement.']\n",
            "['-- Elle se comme de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais d']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 7.\n",
            "Average loss: 1.8227228923719756.\n",
            "157.79596877098083 s elapsed (i.e. 19.724496096372604 s/epoch)\n",
            "[\"-- Quoi? dit-elle vivement; la musique? Ah! mon Dieu, oui! n'ai-je pas ma maison à tenir, mon mari à soigner, mille choses enfin, bien des devoirs qui passent auparavant!\"]\n",
            "['-- Elle se par les comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la co']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 8.\n",
            "Average loss: 1.7897349997442595.\n",
            "176.86263132095337 s elapsed (i.e. 19.65140348010593 s/epoch)\n",
            "['«Messieurs,']\n",
            "['-- Ces comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la co']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 9.\n",
            "Average loss: 1.7526936226961565.\n",
            "196.4179925918579 s elapsed (i.e. 19.64179925918579 s/epoch)\n",
            "['Bouvard la jugeait idiote; et même parla de faire exprès un voyage au Havre.']\n",
            "['-- Ces comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 10.\n",
            "Average loss: 1.7235936717111238.\n",
            "215.03637290000916 s elapsed (i.e. 19.548761172728106 s/epoch)\n",
            "[\"De peur de paraître ridicule, Emma voulut, avant d'entrer, faire un tour de promenade sur le port, et Bovary, par prudence, garda les billets à sa main, dans la poche de son pantalon, qu'il appuyait contre son ventre.\"]\n",
            "['-- Ces comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les comme de la contre les ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 11.\n",
            "Average loss: 1.690974341363323.\n",
            "235.63750553131104 s elapsed (i.e. 19.63645879427592 s/epoch)\n",
            "['-- Tenez!']\n",
            "['-- Elle se partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin de la partin d']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 12.\n",
            "Average loss: 1.663230396280385.\n",
            "255.08979725837708 s elapsed (i.e. 19.622292096798237 s/epoch)\n",
            "['Il alla rôder autour de sa maison. Une lumière brillait dans la cuisine. Il guetta son ombre derrière les rideaux. Rien ne parut.']\n",
            "['-- Celle de la chambres de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la co']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 13.\n",
            "Average loss: 1.6366572793649168.\n",
            "275.03646993637085 s elapsed (i.e. 19.645462138312205 s/epoch)\n",
            "[\"Malgré l'épargne où vivait Bovary, il était loin de pouvoir amortir ses anciennes dettes. Lheureux refusa de renouveler aucun billet. La saisie devint imminente. Alors il eut recours à sa mère, qui consentit à lui laisser prendre une hypothèque sur ses biens, mais en lui envoyant force récriminations contre Emma; et elle demandait, en retour de son sacrifice, un châle, échappé aux ravages de Félicité. Charles le lui refusa. Ils se brouillèrent.\"]\n",
            "['-- Ah! le contre les portes de la pour de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la petit de la ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 14.\n",
            "Average loss: 1.614861294931295.\n",
            "295.21797585487366 s elapsed (i.e. 19.681198390324912 s/epoch)\n",
            "[\"Et, aussitôt, racontant l'histoire de la saisie, elle lui exposa sa détresse; car Charles ignorait tout, sa belle-mère la détestait, le père Rouault ne pouvait rien; mais lui, Léon, il allait se mettre en course pour trouver cette indispensable somme...\"]\n",
            "['-- Ah! la promenait les portes de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pour de la pou']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 15.\n",
            "Average loss: 1.589048260328721.\n",
            "316.4457230567932 s elapsed (i.e. 19.777857691049576 s/epoch)\n",
            "[\"Il exigea l'arriéré d'anciennes visites. On lui montra les lettres que sa femme avait envoyées. Alors il fallut faire des excuses.\"]\n",
            "[\"-- C'est pas de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la \"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 16.\n",
            "Average loss: 1.5689932862106635.\n",
            "336.53080105781555 s elapsed (i.e. 19.79592947398915 s/epoch)\n",
            "[\"Le journal n'inséra pas leur article; le Préfet ne daigna répondre; les Chambres furent muettes, et ils attendirent longtemps un pli du Château. De quoi s'occupait l'Empereur? de femmes sans doute!\"]\n",
            "[\"-- C'est pas de la contre les proches de la conseille de la contre les proches de la conseille de la contre les proches de la conseille de la contre les proches de la conseille de la contre les proches de la conseille de la contre les proches de la conseille de la contre les proches de la conseille de la contre les proches de la conseille de la contre les proches de la conseille de la contre les proches de la conseille de la contre les proches de la conseille de la contre les proches de la conseille de la co\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 17.\n",
            "Average loss: 1.548533056463514.\n",
            "357.53117632865906 s elapsed (i.e. 19.86284312936995 s/epoch)\n",
            "[\"On portait des pantalons de toutes les couleurs, des baudriers crasseux, de vieux habits d'uniforme trop courts, laissant voir la chemise sur les flancs;--et chacun prétendait n'avoir pas le moyen de faire autrement. Une souscription fut ouverte pour habiller les plus pauvres. Foureau lésina, tandis que des femmes se signalèrent. Mme Bordin offrit cinq francs, malgré sa haine de la République. M. de Faverges équipa douze hommes; et ne manquait pas à la manoeuvre. Puis il s'installait chez l'épicier et payait des petits verres au premier venu.\"]\n",
            "['Le constrière de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la p']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 18.\n",
            "Average loss: 1.5313017052047107.\n",
            "377.33684277534485 s elapsed (i.e. 19.859833830281307 s/epoch)\n",
            "['--Vous?']\n",
            "[\"-- C'est pas de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la courure de la cour\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 19.\n",
            "Average loss: 1.5155470894307506.\n",
            "398.0451934337616 s elapsed (i.e. 19.90225967168808 s/epoch)\n",
            "['--Non.']\n",
            "['-- Ah! mais de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la por']\n",
            "BLEU : 0.0\n",
            "lr,emb,hid,lay,B :  (0.02, 128, 256, 2, 32)\n",
            "-- END OF EPOCH 0.\n",
            "Average loss: 3.306658906985055.\n",
            "35.156917095184326 s elapsed (i.e. 35.156917095184326 s/epoch)\n",
            "[\"Après avoir subi ses remerciements, madame Bovary s'en alla; et elle était quelque peu avancée dans le sentier, lorsqu'à un bruit de sabots elle tourna la tête: c'était la nourrice!\"]\n",
            "['e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e e']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 1.\n",
            "Average loss: 2.553441637632798.\n",
            "71.37756204605103 s elapsed (i.e. 35.68878102302551 s/epoch)\n",
            "[\"Comme on savait les origines de Victor et qu'il était désagréable, les autres gamins l'appelaient Forçat; et tout à l'heure il avait flanqué à M. Arnold Marescot une violente raclée. Le cher Arnold en portait des traces sur la figure. Sa mère est au désespoir, son costume en lambeaux, sa santé compromise, où allons-nous?\"]\n",
            "['-- le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 2.\n",
            "Average loss: 2.2007315304814554.\n",
            "106.86557722091675 s elapsed (i.e. 35.621859073638916 s/epoch)\n",
            "[\"-- Mais, si je ne suis pas venu, continua-t-il, si je n'ai pu vous voir, ah! du moins j'ai bien contemplé ce qui vous entoure. La nuit, toutes les nuits, je me relevais, j'arrivais jusqu'ici, je regardais votre maison, le toit qui brillait sous la lune, les arbres du jardin qui se balançaient à votre fenêtre, et une petite lampe, une lueur, qui brillait à travers les carreaux, dans l'ombre. Ah! vous ne saviez guère qu'il y avait là, si près et si loin, un pauvre misérable...\"]\n",
            "['-- Cour le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se par le se']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 3.\n",
            "Average loss: 2.000051674185967.\n",
            "144.46700835227966 s elapsed (i.e. 36.116752088069916 s/epoch)\n",
            "[\"Elle avait prétendu recevoir en dot les Écalles, dont il ne pouvait disposer--l'ayant comme la ferme, soldée en partie avec l'argent d'un autre.\"]\n",
            "['-- Ce se par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les par les ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 4.\n",
            "Average loss: 1.869882879816756.\n",
            "180.22880029678345 s elapsed (i.e. 36.04576005935669 s/epoch)\n",
            "[\"Et elle jurait qu'elle ne se moquait pas, quand un coup de canon retentit; aussitôt, on se poussa, pêle-mêle, vers le village.\"]\n",
            "['-- Charle de la montre de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comm']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 5.\n",
            "Average loss: 1.763010595526014.\n",
            "215.09925484657288 s elapsed (i.e. 35.849875807762146 s/epoch)\n",
            "[\"M. Lieuvain se rassit alors; M. Derozerays se leva, commençant un autre discours. Le sien peut-être, ne fut point aussi fleuri que celui du Conseiller; mais il se recommandait par un caractère de style plus positif, c'est-à-dire par des connaissances plus spéciales et des considérations plus relevées. Ainsi, l'éloge du gouvernement y tenait moins de place; la religion et l'agriculture en occupaient davantage. On y voyait le rapport de l'une et de l'autre, et comment elles avaient concouru toujours à la civilisation. Rodolphe, avec madame Bovary, causait rêves, pressentiments, magnétisme. Remontant au berceau des sociétés, l'orateur vous dépeignait ces temps farouches où les hommes vivaient de glands, au fond des bois. Puis ils avaient quitté la dépouille des bêtes; endossé le drap, creusé des sillons, planté la vigne. Était-ce un bien, et n'y avait-il pas dans cette découverte plus d'inconvénients que d'avantages? M. Derozerays se posait ce problème. Du magnétisme, peu à peu, Rodolphe en était venu aux affinités, et, tandis que M. le président citait Cincinnatus à sa charrue, Dioclétien plantant ses choux, et les empereurs de la Chine inaugurant l'année par des semailles, le jeune homme expliquait à la jeune femme que ces attractions irrésistibles tiraient leur cause de quelque existence antérieure.\"]\n",
            "['-- Ce se perrent de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 6.\n",
            "Average loss: 1.6750174204105048.\n",
            "251.14013481140137 s elapsed (i.e. 35.87716211591448 s/epoch)\n",
            "[\"Ils essayèrent d'imiter Sanctorius. Mais comme leur balance ne pouvait les supporter tous les deux, ce fut Pécuchet qui commença.\"]\n",
            "[\"-- Ah! n'est pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait pas de la contenait \"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 7.\n",
            "Average loss: 1.5977015994033035.\n",
            "287.08353424072266 s elapsed (i.e. 35.88544178009033 s/epoch)\n",
            "[\"Alors, elle se livra à des charités excessives. Elle cousait des habits pour les pauvres; elle envoyait du bois aux femmes en couches; et Charles, un jour en rentrant, trouva dans la cuisine trois vauriens attablés qui mangeaient un potage. Elle fit revenir à la maison sa petite fille, que son mari, durant sa maladie, avait renvoyée chez la nourrice. Elle voulut lui apprendre à lire; Berthe avait beau pleurer, elle ne s'irritait plus. C'était un parti pris de résignation, une indulgence universelle. Son langage, à propos de tout, était plein d'expressions idéales. Elle disait à son enfant:\"]\n",
            "[\"-- Ah! n'est pas de la connaissait des consesses de la connaissait des consesses de la connaissait des consesses de la connaissait des consesses de la connaissait des consesses de la connaissait des consesses de la connaissait des consesses de la connaissait des consesses de la connaissait des consesses de la connaissait des consesses de la connaissait des consesses de la connaissait des consesses de la connaissait des consesses de la connaissait des consesses de la connaissait des consesses de la connaissai\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 8.\n",
            "Average loss: 1.5315687127259312.\n",
            "323.7556450366974 s elapsed (i.e. 35.972849448521934 s/epoch)\n",
            "[\"-- Mais, si vous n'avez pas d'espèces, vous avez du bien.\"]\n",
            "[\"-- Ah! n'ai vous pas de la couper de la courte de la couper de la courte de la couper de la courte de la couper de la courte de la couper de la courte de la couper de la courte de la couper de la courte de la couper de la courte de la couper de la courte de la couper de la courte de la couper de la courte de la couper de la courte de la couper de la courte de la couper de la courte de la couper de la courte de la couper de la courte de la couper de la courte de la couper de la courte de la couper de la court\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 9.\n",
            "Average loss: 1.4884742157799857.\n",
            "359.1652157306671 s elapsed (i.e. 35.91652157306671 s/epoch)\n",
            "['Il se rassit, croisa les bras. Vous êtes une rude canaille, convenez-en!']\n",
            "[\"-- C'est pas de son contre les contresses de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de la continuaire de \"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 10.\n",
            "Average loss: 1.441217276514793.\n",
            "395.75615334510803 s elapsed (i.e. 35.97783212228255 s/epoch)\n",
            "[\"Et, bien qu'il connût ce pauvre diable, il feignit de le voir pour la première fois, murmura les mots de cornée, cornée opaque, sclérotique, faciès, puis lui demanda d'un ton paterne:\"]\n",
            "['-- Ah! moi disportait les cours de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 11.\n",
            "Average loss: 1.4059703970442012.\n",
            "430.46805334091187 s elapsed (i.e. 35.872337778409324 s/epoch)\n",
            "['--Farceurs! je sais bien que vous le cachez!']\n",
            "['-- Ah! vous mon vous pas de la chercher les considérations de la charrette, et le considérait le conseil de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charriotion de la charri']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 12.\n",
            "Average loss: 1.3693412185320395.\n",
            "466.6018269062042 s elapsed (i.e. 35.89244822355417 s/epoch)\n",
            "[\"Ils avaient fait venir un serrurier pour les tuteurs, un quincaillier pour les raidisseurs, un charpentier pour les supports. Les formes des arbres étaient d'avance dessinées. Des morceaux de latte sur le mur figuraient des candélabres. Deux poteaux à chaque bout des plates-bandes guindaient horizontalement des fils de fer;--et dans le verger, des cerceaux indiquaient la structure des vases, des baguettes en cône celle des pyramides--si bien qu'en arrivant chez eux, on croyait voir les pièces de quelque machine inconnue, ou la carcasse d'un feu d'artifice.\"]\n",
            "['-- On ait disparut de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller d']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 13.\n",
            "Average loss: 1.3438250428559828.\n",
            "502.71616888046265 s elapsed (i.e. 35.9082977771759 s/epoch)\n",
            "[\"-- Tout à l'heure! je sors. Nous irons au Fanal de Rouen, voir ces messieurs. Je vous présenterai à Thomassin.\"]\n",
            "['-- Ah! le soir, de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la c']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 14.\n",
            "Average loss: 1.3238984516688757.\n",
            "537.9965815544128 s elapsed (i.e. 35.86643877029419 s/epoch)\n",
            "[\"--Je n'y crois pas, au règne minéral! puisque des matières organiques ont pris part à la formation du silex, de la craie, de l'or peut-être! Le diamant n'a-t-il pas été du charbon: la houille un assemblage de végétaux:--en la chauffant à je ne sais plus combien de degrés, on obtient de la sciure de bois, tellement que tout passe, tout coule. La création est faite d'une matière ondoyante et fugace. Mieux vaudrait nous occuper d'autre chose!\"]\n",
            "['-- Ah! mais dit Bouvard, et le premier de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille de sa petite fille']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 15.\n",
            "Average loss: 1.3090296442411384.\n",
            "577.7349779605865 s elapsed (i.e. 36.10843612253666 s/epoch)\n",
            "[\"Car elle redoutait qu'on ne fût à la questionner, à la soigner, qu'on ne la quittât plus.\"]\n",
            "[\"-- Ah! vous avez vous de l'autre fois de la cheminée de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller de la conseiller d\"]\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 16.\n",
            "Average loss: 1.2933616814564686.\n",
            "613.8265435695648 s elapsed (i.e. 36.10744373938616 s/epoch)\n",
            "[\"La journée fut longue, le lendemain! Elle se promena dans son jardinet, passant et revenant par les mêmes allées, s'arrêtant devant les plates-bandes, devant l'espalier, devant le curé de plâtre, considérant avec ébahissement toutes ces choses d'autrefois qu'elle connaissait si bien. Comme le bal déjà lui semblait loin! Qui donc écartait, à tant de distance, le matin d'avant-hier et le soir d'aujourd'hui? Son voyage à la Vaubyessard avait fait un trou dans sa vie, à la manière de ces grandes crevasses qu'un orage, en une seule nuit, creuse quelquefois dans les montagnes. Elle se résigna pourtant; elle serra pieusement dans la commode sa belle toilette et jusqu'à ses souliers de satin, dont la semelle s'était jaunie à la cire glissante du parquet. Son coeur était comme eux: au frottement de la richesse, il s'était placé dessus quelque chose qui ne s'effacerait pas.\"]\n",
            "['-- Ah! moi! dit le mari de la chambre de la chambre de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 17.\n",
            "Average loss: 1.2679629052171901.\n",
            "649.4658401012421 s elapsed (i.e. 36.081435561180115 s/epoch)\n",
            "['-- Non! dit Emma.']\n",
            "[\"-- Ah! c'est un moins de la cour, et le marchand de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 18.\n",
            "Average loss: 1.256600917602072.\n",
            "685.5840353965759 s elapsed (i.e. 36.08337028403031 s/epoch)\n",
            "[\"On envoya bien vite prendre des pigeons au Lion d'or, tout ce qu'il y avait de côtelettes à la boucherie, de la crème chez Tuvache, des oeufs chez Lestiboudois, et l'apothicaire aidait lui-même aux préparatifs, tandis que madame Homais disait, en tirant les cordons de sa camisole:\"]\n",
            "['-- Ah! non! disait-il en se mit à ce qui avait pas de la main de la main de la main de la main de la main de la main de la chambre, et le marchand de la conseilla de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagnie de la compagni']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 19.\n",
            "Average loss: 1.2502677289362487.\n",
            "721.4139125347137 s elapsed (i.e. 36.070695626735684 s/epoch)\n",
            "[\"-- J'aurais besoin d'un manteau, un grand manteau, à long collet, doublé.\"]\n",
            "['-- Ah! mais par le monde de la chambre, de la chambre, de la chambre, le compte de la chambre, le considération de la convenace de la convenace de la convenace de la convenace, et le conseilla de la convenace de la convenace, et le conseilla de la convenace de la convenace, et le conseilla de la convenace de la convenace, et le conseilla de la convenace de la convenace, et le conseilla de la convenace de la convenace, et le conseilla de la convenace de la convenace, et le conseilla de la convenace de la conv']\n",
            "BLEU : 0.0\n",
            "lr,emb,hid,lay,B :  (0.02, 128, 256, 2, 64)\n",
            "-- END OF EPOCH 0.\n",
            "Average loss: 3.6345572784693556.\n",
            "29.487506866455078 s elapsed (i.e. 29.487506866455078 s/epoch)\n",
            "['Alors il fit la proposition de sortir du théâtre, pour aller prendre des glaces quelque part.']\n",
            "['  aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa aa a']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 1.\n",
            "Average loss: 2.9679956192872963.\n",
            "59.72208333015442 s elapsed (i.e. 29.86104166507721 s/epoch)\n",
            "[\"À huit heures, Justin venait le chercher pour fermer la pharmacie. Alors M. Homais le regardait d'un oeil narquois, surtout si Félicité se trouvait là, s'étant aperçu que son élève affectionnait la maison du médecin.\"]\n",
            "['e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e le e l']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 2.\n",
            "Average loss: 2.661952573425916.\n",
            "88.4042432308197 s elapsed (i.e. 29.4680810769399 s/epoch)\n",
            "['-- Ne pleure pas! lui dit-elle. Bientôt je ne te tourmenterai plus!']\n",
            "['- le sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant de sant']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 3.\n",
            "Average loss: 2.41974430911395.\n",
            "118.88531923294067 s elapsed (i.e. 29.72132980823517 s/epoch)\n",
            "[\"Puis, se calmant, elle finit par découvrir qu'elle l'avait sans doute calomnié. Mais le dénigrement de ceux que nous aimons toujours nous en détache quelque peu. Il ne faut pas toucher aux idoles: la dorure en reste aux mains.\"]\n",
            "['-- le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 4.\n",
            "Average loss: 2.2453134765430374.\n",
            "148.65469694137573 s elapsed (i.e. 29.730939388275146 s/epoch)\n",
            "[\"Le vestibule où trois domestiques les attendaient pour prendre leurs paletots, le billard et les deux salons en enfilade, les plantes dans les vases de la Chine, les bronzes sur les cheminées, les baguettes d'or aux lambris, les rideaux épais, les larges fauteuils, ce luxe immédiatement les flatta comme une politesse qu'on leur faisait;--et en entrant dans la salle à manger, au spectacle de la table couverte de viandes sur les plats d'argent, avec la rangée des verres devant chaque assiette, les hors d'oeuvre çà et là, et un saumon au milieu, tous les visages s'épanouirent.\"]\n",
            "['---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 5.\n",
            "Average loss: 2.1239352980438544.\n",
            "177.90993881225586 s elapsed (i.e. 29.65165646870931 s/epoch)\n",
            "[\"-- Serviteur, madame, reprit-il d'un ton sec.\"]\n",
            "['-- Cour le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de le se pars de l']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 6.\n",
            "Average loss: 2.03086356362518.\n",
            "207.33627080917358 s elapsed (i.e. 29.61946725845337 s/epoch)\n",
            "[\"--Eh bien, les confrères, comment va l'anatomie?\"]\n",
            "['-- Elle par le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 7.\n",
            "Average loss: 1.950565909852787.\n",
            "236.26434326171875 s elapsed (i.e. 29.533042907714844 s/epoch)\n",
            "[\"Ils perfectionnèrent les achars de Mme Bordin, en épiçant le vinaigre avec du poivre; et leurs prunes à l'eau-de-vie étaient bien supérieures! Ils obtinrent par la macération des ratafias de framboise et d'absinthe. Avec du miel et de l'angélique dans un tonneau de Bagnols, ils voulurent faire du vin de Malaga; et ils entreprirent également la confection d'un champagne! Les bouteilles de chablis, coupées de moût, éclatèrent d'elles-mêmes. Alors, ils ne doutèrent plus de la réussite.\"]\n",
            "['-- Ce la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de la charis de ']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 8.\n",
            "Average loss: 1.8797040800659024.\n",
            "265.2203812599182 s elapsed (i.e. 29.468931251102024 s/epoch)\n",
            "['-- Où vous voudrez.']\n",
            "['-- Ce le se comme de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde de la charde d']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 9.\n",
            "Average loss: 1.8269740805333974.\n",
            "294.53902864456177 s elapsed (i.e. 29.453902864456175 s/epoch)\n",
            "[\"Elle roulait sa tête avec un geste doux plein d'angoisse, et tout en ouvrant continuellement les mâchoires, comme si elle eût porté sur sa langue quelque chose de très lourd. À huit heures, les vomissements reparurent.\"]\n",
            "['-- Ah se partion de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de l']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 10.\n",
            "Average loss: 1.7666227014697329.\n",
            "323.5508871078491 s elapsed (i.e. 29.413717009804465 s/epoch)\n",
            "['Le pharmacien répondit:']\n",
            "['-- Charles de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les partit de la comme les par']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 11.\n",
            "Average loss: 1.7199094575278613.\n",
            "352.7088141441345 s elapsed (i.e. 29.392401178677876 s/epoch)\n",
            "['_Soyons heureux! buvons! car la coupe est remplie,_ _Car cette heure est à nous, et le reste est folie._']\n",
            "['-- Ce se trouvait de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 12.\n",
            "Average loss: 1.6716751113082424.\n",
            "382.9015119075775 s elapsed (i.e. 29.45396245442904 s/epoch)\n",
            "['-- Ah bah! quand on a comme vous des amis!']\n",
            "['-- Ah! comme un contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la con']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 13.\n",
            "Average loss: 1.628666416722901.\n",
            "412.18812322616577 s elapsed (i.e. 29.442008801868983 s/epoch)\n",
            "[\"-- Cela vous semble drôle, n'est-ce pas? moi qui reste toujours plus confiné dans mon laboratoire que le rat du bonhomme dans son fromage.\"]\n",
            "['-- Ah! mais le courir de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire de la courire d']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 14.\n",
            "Average loss: 1.5881462547243859.\n",
            "440.6131258010864 s elapsed (i.e. 29.374208386739095 s/epoch)\n",
            "[\"Il fallait lui servir quelque chose de bien. Bouvard était partisan de _La Tour de Nesle_. Mais Pécuchet avait peur des rôles qui demandent trop d'action.\"]\n",
            "['-- Ah! comme un contre les consieurs de la contre les consieurs de la contre les consieurs de la contre les consieurs de la contre les consieurs de la contre les consieurs de la contre les consieurs de la contre les consieurs de la contre les consieurs de la contre les consieurs de la contre les consieurs de la contre les consieurs de la contre les consieurs de la contre les consieurs de la contre les consieurs de la contre les consieurs de la contre les consieurs de la contre les consieurs de la contre les ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 15.\n",
            "Average loss: 1.5527202730276146.\n",
            "470.9800064563751 s elapsed (i.e. 29.436250403523445 s/epoch)\n",
            "['Un mois se passa dans le désoeuvrement. Puis ils songèrent à leur jardin.']\n",
            "['-- Ah! comme un couvert de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la courre de la co']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 16.\n",
            "Average loss: 1.5196307350178153.\n",
            "501.10312151908875 s elapsed (i.e. 29.47665420700522 s/epoch)\n",
            "[\"Charles vint l'embrasser sur l'épaule.\"]\n",
            "[\"-- Ce n'est pas de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 17.\n",
            "Average loss: 1.4866920089235112.\n",
            "531.1735663414001 s elapsed (i.e. 29.509642574522232 s/epoch)\n",
            "['Le maire la lutinait. Elle ripostait aux plaisanteries. Ensuite elle indiqua une recette pour les cornichons. Du reste, ses talents de ménagère étaient connus, et elle avait une petite ferme admirablement soignée.']\n",
            "[\"-- C'est ce qu'il en avait pas de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cou\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 18.\n",
            "Average loss: 1.4594891837665014.\n",
            "561.868577003479 s elapsed (i.e. 29.572030368604157 s/epoch)\n",
            "[\"Rien pourtant ne la forçait à partir; mais elle avait donné sa parole qu'elle reviendrait le soir même. D'ailleurs, Charles l'attendait; et déjà elle se sentait au coeur cette lâche docilité qui est, pour bien des femmes, comme le châtiment tout à la fois et la rançon de l'adultère.\"]\n",
            "['-- Ah! ce qui avait pas de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la com']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 19.\n",
            "Average loss: 1.4405510036312803.\n",
            "592.0192019939423 s elapsed (i.e. 29.60096009969711 s/epoch)\n",
            "[\"Du temps de madame Dubuc, la vieille femme se sentait encore la préférée; mais, à présent, l'amour de Charles pour Emma lui semblait une désertion de sa tendresse, un envahissement sur ce qui lui appartenait; et elle observait le bonheur de son fils avec un silence triste, comme quelqu'un de ruiné qui regarde, à travers les carreaux, des gens attablés dans son ancienne maison. Elle lui rappelait, en manière de souvenirs, ses peines et ses sacrifices, et, les comparant aux négligences d'Emma, concluait qu'il n'était point raisonnable de l'adorer d'une façon si exclusive.\"]\n",
            "['-- Comment de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courrir de la courri']\n",
            "BLEU : 0.0\n",
            "lr,emb,hid,lay,B :  (0.02, 128, 256, 3, 32)\n",
            "-- END OF EPOCH 0.\n",
            "Average loss: 3.410914960851524.\n",
            "50.58030986785889 s elapsed (i.e. 50.58030986785889 s/epoch)\n",
            "['--Pas de blagues! Crois-tu oui, ou non?']\n",
            "['                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 1.\n",
            "Average loss: 3.0308101201544004.\n",
            "100.81329607963562 s elapsed (i.e. 50.40664803981781 s/epoch)\n",
            "[\"-- Pensiez-vous, ma petite dame, que j'allais, jusqu'à la consommation des siècles, être votre fournisseur et banquier pour l'amour de Dieu? Il faut bien que je rentre dans mes déboursés, soyons justes!\"]\n",
            "['--e le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 2.\n",
            "Average loss: 2.753232007123986.\n",
            "149.17819046974182 s elapsed (i.e. 49.72606348991394 s/epoch)\n",
            "['Elle eut la délicatesse de le reconduire,--escortée de Marianne, qui portait un falot.']\n",
            "['-- aus le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le de de le']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 3.\n",
            "Average loss: 2.4354923513470865.\n",
            "198.3201162815094 s elapsed (i.e. 49.58002907037735 s/epoch)\n",
            "['-- Ah! merci, dit Charles, vous êtes bon!']\n",
            "[\"-- l'ant de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pou\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 4.\n",
            "Average loss: 2.1803785976098506.\n",
            "250.16439819335938 s elapsed (i.e. 50.03287963867187 s/epoch)\n",
            "[\"--Je tâte votre habit, l'étoffe en est moelleuse. Et il dardait ses prunelles, tendait la bouche, reniflait, avait un air extrêmement lubrique, finit même par s'adresser à Mme Bordin.\"]\n",
            "['-- Elle les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de les couter de l']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 5.\n",
            "Average loss: 2.007008711294252.\n",
            "297.2320713996887 s elapsed (i.e. 49.538678566614784 s/epoch)\n",
            "[\"Néanmoins, ils donnaient des conseils, remontaient le moral, avaient l'audace d'ausculter.\"]\n",
            "['-- Ces par le conter de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le mais de le ma']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 6.\n",
            "Average loss: 1.8654827714571494.\n",
            "347.8046543598175 s elapsed (i.e. 49.686379194259644 s/epoch)\n",
            "[\"Pécuchet le matin du même jour s'était promis de mourir, s'il n'obtenait pas les faveurs de sa bonne--et il l'avait accompagnée dans la cave, espérant que les ténèbres lui donneraient de l'audace.\"]\n",
            "['-- Charme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de la contre de la comme de ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 7.\n",
            "Average loss: 1.7457184457049078.\n",
            "397.4352910518646 s elapsed (i.e. 49.67941138148308 s/epoch)\n",
            "[\"Il y eut une agitation sur l'estrade, de longs chuchotements, des pourparlers. Enfin, M. le Conseiller se leva. On savait maintenant qu'il s'appelait Lieuvain, et l'on se répétait son nom de l'un à l'autre, dans la foule. Quand il eut donc collationné quelques feuilles et appliqué dessus son oeil pour y mieux voir, il commença:\"]\n",
            "['-- Elle se pas de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de la courir de l']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 8.\n",
            "Average loss: 1.65587474375355.\n",
            "447.626487493515 s elapsed (i.e. 49.736276388168335 s/epoch)\n",
            "[\"Et, ramassant un catéchisme en lambeaux qu'il venait de heurter avec son pied:\"]\n",
            "[\"-- Ce l'est pas de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de \"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 9.\n",
            "Average loss: 1.5809234459789432.\n",
            "496.1383502483368 s elapsed (i.e. 49.613835024833676 s/epoch)\n",
            "[\"Gorju s'emporta contre la lourdeur de tous ces gars de la campagne puis, à genoux devant le meuble, il cherchait la place du morceau. Pécuchet en voulant l'aider, distingua sous la poussière, des figures de personnages.\"]\n",
            "['-- Ah! mais le courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait de la courait ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 10.\n",
            "Average loss: 1.5166972024100167.\n",
            "545.9905843734741 s elapsed (i.e. 49.63550767031583 s/epoch)\n",
            "[\"Charles, sollicité par l'apothicaire et par elle, se laissa convaincre. Il fit venir de Rouen le volume du docteur Duval, et, tous les soirs, se prenant la tête entre les mains, il s'enfonçait dans cette lecture.\"]\n",
            "['-- Comment de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la contraire de la ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 11.\n",
            "Average loss: 1.4635659626552038.\n",
            "593.8140518665314 s elapsed (i.e. 49.484504322210945 s/epoch)\n",
            "['Enfin des poires parurent; et le verger avait des prunes. Alors ils employèrent contre les oiseaux tous les artifices recommandés. Mais les fragments de glace miroitaient à éblouir, la cliquette du moulin à vent les réveillait pendant la nuit--et les moineaux perchaient sur le mannequin. Ils en firent un second, et même un troisième, dont ils varièrent le costume, inutilement.']\n",
            "[\"-- Ce qu'il ne pas de manier de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de l\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 12.\n",
            "Average loss: 1.4271328261661045.\n",
            "644.6697745323181 s elapsed (i.e. 49.58998265633216 s/epoch)\n",
            "['Ils tirèrent au sort pour savoir qui se présenterait. Le sort ne trancha rien--et ils allèrent consulter là-dessus, le docteur.']\n",
            "['-- Comment de la champs de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 13.\n",
            "Average loss: 1.3872303068637848.\n",
            "693.6311745643616 s elapsed (i.e. 49.5450838974544 s/epoch)\n",
            "[\"Avons-nous dans notre idiome des articles définis et indéfinis comme en latin? Les uns pensent que oui, les autres que non. Ils n'osèrent se décider.\"]\n",
            "['-- Cela de la partie de la partie de la partie de la partie de la porte de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie de la partie']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 14.\n",
            "Average loss: 1.3563499444601488.\n",
            "740.9607236385345 s elapsed (i.e. 49.3973815759023 s/epoch)\n",
            "[\"Il lui montra la lettre où sa mère narrait l'événement, sans aucune hypocrisie sentimentale. Seulement, elle regrettait que son mari n'eût pas reçu les secours de la religion, étant mort à Doudeville, dans la rue, sur le seuil d'un café, après un repas patriotique avec d'anciens officiers.\"]\n",
            "['-- Ah! mais il se retrouvait de la cour.']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 15.\n",
            "Average loss: 1.326922441623649.\n",
            "790.8916811943054 s elapsed (i.e. 49.43073007464409 s/epoch)\n",
            "[\"Outre la compagnie de sa belle-mère, qui la raffermissait un peu par sa rectitude de jugement et ses façons graves, Emma, presque tous les jours, avait encore d'autres sociétés. C'était madame Langlois, madame Caron, madame Dubreuil, madame Tuvache et, régulièrement, de deux à cinq heures, l'excellente madame Homais, qui n'avait jamais voulu croire, celle-là, à aucun des cancans que l'on débitait sur sa voisine. Les petits Homais aussi venaient la voir; Justin les accompagnait. Il montait avec eux dans la chambre, et il restait debout près de la porte, immobile, sans parler. Souvent même, madame Bovary, n'y prenant garde, se mettait à sa toilette. Elle commençait par retirer son peigne, en secouant sa tête d'un mouvement brusque; et, quand il aperçut la première fois cette chevelure entière qui descendait jusqu'aux jarrets en déroulant ses anneaux noirs, ce fut pour lui, le pauvre enfant, comme l'entrée subite dans quelque chose d'extraordinaire et de nouveau dont la splendeur l'effraya.\"]\n",
            "['-- Ah! mais tout en pas de la contraire de la contre le premier de la contre le premier de la contre les plus tards de la contraire.']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 16.\n",
            "Average loss: 1.3051119604889228.\n",
            "839.8944938182831 s elapsed (i.e. 49.405558459899005 s/epoch)\n",
            "[\"Cependant Rodolphe, avec madame Bovary, était monté au premier étage de la mairie, dans la salle des délibérations, et, comme elle était vide, il avait déclaré que l'on y serait bien pour jouir du spectacle plus à son aise. Il prit trois tabourets autour de la table ovale, sous le buste du monarque, et, les ayant approchés de l'une des fenêtres, ils s'assirent l'un près de l'autre.\"]\n",
            "['-- Ah! monsieur de la conversation de la conservent.']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 17.\n",
            "Average loss: 1.2877749253292472.\n",
            "889.3691346645355 s elapsed (i.e. 49.409396370251976 s/epoch)\n",
            "[\"Rodolphe, l'ayant aperçu de loin, avait pris un train rapide; mais madame Bovary s'essouffla; il se ralentit donc et lui dit en souriant, d'un ton brutal:\"]\n",
            "['-- Ah! monsieur de la main de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 18.\n",
            "Average loss: 1.2721028917906236.\n",
            "939.2430913448334 s elapsed (i.e. 49.43384691288597 s/epoch)\n",
            "[\"Le choix l'embarrassait. Elle n'avait vu que trois pièces: _Robert le Diable_ dans la capitale, le _Jeune Mari_ à Rouen--et une autre à Falaise qui était bien amusante et qu'on appelait _La Brouette du Vinaigrier_.\"]\n",
            "[\"-- Ah! c'est pas de la conduite de la contemple de la contemple de la contemple.\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 19.\n",
            "Average loss: 1.2530753618569543.\n",
            "989.469979763031 s elapsed (i.e. 49.47349898815155 s/epoch)\n",
            "[\"Larsonneur, qu'ils instruisirent du fait, n'en voulut rien croire.\"]\n",
            "['-- Ah! dit le courant de la conversation.']\n",
            "BLEU : 0.0\n",
            "lr,emb,hid,lay,B :  (0.02, 128, 256, 3, 64)\n",
            "-- END OF EPOCH 0.\n",
            "Average loss: 3.668530888027615.\n",
            "40.10036659240723 s elapsed (i.e. 40.10036659240723 s/epoch)\n",
            "[\"-- Y a-t-il longtemps, mon ami, que tu as cette épouvantable infirmité? Au lieu de t'enivrer au cabaret, tu ferais mieux de suivre un régime.\"]\n",
            "['  eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 1.\n",
            "Average loss: 3.1461076931077607.\n",
            "79.26690912246704 s elapsed (i.e. 39.63345456123352 s/epoch)\n",
            "[\"Leurs acquisitions furent distribuées dans tous les appartements. Une crèche remplie de foin et une cathédrale de liège décorèrent le muséum. Il y eut sur la cheminée de Pécuchet, un saint Jean-Baptiste en cire, le long du corridor les portraits des gloires épiscopales, et au bas de l'escalier, sous une lampe à chaînettes, une sainte Vierge en manteau d'azur et couronnée d'étoiles--Marcel nettoyait ces splendeurs, n'imaginant au paradis rien de plus beau.\"]\n",
            "['                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 ']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 2.\n",
            "Average loss: 3.092261956662548.\n",
            "118.7233567237854 s elapsed (i.e. 39.5744522412618 s/epoch)\n",
            "[\"Mélie dans la cour, tirait de l'eau.\"]\n",
            "['-  aaa  aaa  aa aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa a aa']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 3.\n",
            "Average loss: 2.9807649850845337.\n",
            "159.1674542427063 s elapsed (i.e. 39.791863560676575 s/epoch)\n",
            "[\"Elle aurait voulu pouvoir surveiller sa vie, et l'idée lui vint de le faire suivre dans les rues. Il y avait toujours, près de l'hôtel, une sorte de vagabond qui accostait les voyageurs et qui ne refuserait pas... Mais sa fierté se révolta.\"]\n",
            "['--e e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue e ue']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 4.\n",
            "Average loss: 2.810123988560268.\n",
            "199.8485233783722 s elapsed (i.e. 39.969704675674436 s/epoch)\n",
            "[\"Justin, qui apportait alors une pile d'assiettes, fut saisi d'un tremblement.\"]\n",
            "['-- e le le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de le de l']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 5.\n",
            "Average loss: 2.611089689391.\n",
            "240.2667155265808 s elapsed (i.e. 40.044452587763466 s/epoch)\n",
            "[\"Au milieu, sur une chaufferette, de l'encens fumait. Bouvard se tenait derrière--et Pécuchet, lui tournant le dos, jetait dans l'âtre des poignées de soufre.\"]\n",
            "['-- ou le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 6.\n",
            "Average loss: 2.4652655659889686.\n",
            "279.8146755695343 s elapsed (i.e. 39.97352508136204 s/epoch)\n",
            "[\"Et dans le rire dont Bouvard fut pris, ses épaules et son ventre sautaient d'accord. Plus rouge que les confitures, avec sa serviette sous l'aisselle, il répétait: Ah! ah! ah! d'une façon irritante.\"]\n",
            "['-- le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le ']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 7.\n",
            "Average loss: 2.3361951453345164.\n",
            "319.59778666496277 s elapsed (i.e. 39.949723333120346 s/epoch)\n",
            "[\"Et, tandis qu'il rebouclait son carton, il discourait ainsi sur la clientèle du médecin.\"]\n",
            "[\"-- l'ant le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le \"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 8.\n",
            "Average loss: 2.2287974041335437.\n",
            "360.1864855289459 s elapsed (i.e. 40.020720614327324 s/epoch)\n",
            "[\"Un soir que la fenêtre était ouverte, et que, assise au bord, elle venait de regarder Lestiboudois, le bedeau, qui taillait le buis, elle entendit tout à coup sonner l'Angelus.\"]\n",
            "[\"-- l'ante de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le pour de le po\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 9.\n",
            "Average loss: 2.1336557037976323.\n",
            "399.38078594207764 s elapsed (i.e. 39.93807859420777 s/epoch)\n",
            "[\"--L'Inquisition employait de même la torture, et elle vous brûlait très bien.\"]\n",
            "['-- Au le pars de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le parte de le p']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 10.\n",
            "Average loss: 2.0501352864868787.\n",
            "439.1638367176056 s elapsed (i.e. 39.92398515614596 s/epoch)\n",
            "['Pécuchet baissa la tête.']\n",
            "['-- Cour de le cours de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait de le prait d']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 11.\n",
            "Average loss: 1.972125417115737.\n",
            "477.6458282470703 s elapsed (i.e. 39.803819020589195 s/epoch)\n",
            "[\"Vaucorbeil prit cette parole pour une allusion à l'herpès de Mme Bordin, histoire clabaudée par la veuve, et dont le souvenir l'agaçait.\"]\n",
            "['-- Cour de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme de le comme d']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 12.\n",
            "Average loss: 1.898681970557781.\n",
            "515.9444692134857 s elapsed (i.e. 39.688036093345055 s/epoch)\n",
            "['-- Il était capitaine de vaisseau, mon ami.']\n",
            "['-- Con de la charins de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la contendre de la conten']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 13.\n",
            "Average loss: 1.8294106040682112.\n",
            "554.8863666057587 s elapsed (i.e. 39.634740471839905 s/epoch)\n",
            "['-- Je vendrai encore...']\n",
            "['-- Con en avait le proment de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 14.\n",
            "Average loss: 1.7739298793734337.\n",
            "592.8534045219421 s elapsed (i.e. 39.52356030146281 s/epoch)\n",
            "[\"Après quoi, les propriétaires suivant l'usage offrirent de casser une croûte à la maison; et Pécuchet ouvrit une des bouteilles de son malaga, moins par générosité que dans l'espoir d'en obtenir des éloges.\"]\n",
            "['-- Cont de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contr']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 15.\n",
            "Average loss: 1.7139357437892837.\n",
            "633.0736148357391 s elapsed (i.e. 39.567100927233696 s/epoch)\n",
            "[\"-- «Malgré les préjugés qui recouvrent encore une partie de la face de l'Europe comme un réseau, la lumière cependant commence à pénétrer dans nos campagnes. C'est ainsi que, mardi, notre petite cité d'Yonville s'est vue le théâtre d'une expérience chirurgicale qui est en même temps un acte de haute philanthropie. M. Bovary, un de nos praticiens les plus distingués...»\"]\n",
            "['-- Cont de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charre de la charr']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 16.\n",
            "Average loss: 1.6644633862437035.\n",
            "673.9694421291351 s elapsed (i.e. 39.64526130171383 s/epoch)\n",
            "[\"-- C'est une fille! dit Charles.\"]\n",
            "['-- Ah! de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couverte de la couve']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 17.\n",
            "Average loss: 1.6227165971483504.\n",
            "712.9086103439331 s elapsed (i.e. 39.606033907996284 s/epoch)\n",
            "[\"Madame Bovary n'avait pas encore l'intelligence assez nette pour s'appliquer sérieusement à n'importe quoi; d'ailleurs, elle entreprit ces lectures avec trop de précipitation. Elle s'irrita contre les prescriptions du culte; l'arrogance des écrits polémiques lui déplut par leur acharnement à poursuivre des gens qu'elle ne connaissait pas; et les contes profanes relevés de religion lui parurent écrits dans une telle ignorance du monde, qu'ils l'écartèrent insensiblement des vérités dont elle attendait la preuve. Elle persista pourtant, et, lorsque le volume lui tombait des mains, elle se croyait prise par la plus fine mélancolie catholique qu'une âme éthérée pût concevoir.\"]\n",
            "['-- Ah! de la porte de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenait de la promenai']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 18.\n",
            "Average loss: 1.5787111800544116.\n",
            "752.7187595367432 s elapsed (i.e. 39.61677681772333 s/epoch)\n",
            "['Devenus plus hardis, ils dévastaient le jardin. Mais quel amusement leur donner?']\n",
            "[\"-- Ah! ce n'est pas de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe de la coupe d\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 19.\n",
            "Average loss: 1.5455516966021792.\n",
            "792.9620740413666 s elapsed (i.e. 39.648103702068326 s/epoch)\n",
            "['Pécuchet lui mit la main droite dans la main gauche de Victoire--et les cils toujours clos, les pommettes un peu rouges, les lèvres frémissantes, la somnambule, après avoir divagué, ordonna du Valum Becum.']\n",
            "[\"-- Ce n'est pas de sa courir de la champte de sa couper de la champte de sa couper de la champte de sa couper de la champte de sa couper de la champte de sa couper de la champte de sa couper de la champte de sa couper de la champte de sa couper de la champte de sa couper de la champte de sa couper de la champte de sa couper de la champte de sa couper de la champte de sa couper de la champte de sa couper de la champte de sa couper de la champte de sa couper de la champte de sa couper de la champte de sa coupe\"]\n",
            "BLEU : 0.0\n",
            "lr,emb,hid,lay,B :  (0.02, 128, 512, 1, 32)\n",
            "-- END OF EPOCH 0.\n",
            "Average loss: 2.873134508955902.\n",
            "43.26430153846741 s elapsed (i.e. 43.26430153846741 s/epoch)\n",
            "[\"Les deux hommes se récrièrent; et un dialogue s'en suivit sur les femmes, sur l'amour. Marescot affirma qu'il existe beaucoup d'unions heureuses.--Parfois même, sans qu'on s'en doute, on a près de soi, ce qu'il faudrait pour son bonheur. L'allusion était directe. Les joues de la veuve s'empourprèrent; mais se remettant presque aussitôt:\"]\n",
            "['---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 1.\n",
            "Average loss: 2.1774091002892475.\n",
            "87.48012447357178 s elapsed (i.e. 43.74006223678589 s/epoch)\n",
            "[\"Le curé harangua les petits garçons. Qu'ils aient soin plus tard de ne point faire comme Judas qui trahit son Dieu, et de conserver toujours leur robe d'innocence. Pécuchet regretta la sienne. Mais on remuait des chaises; les mères avaient hâte d'embrasser leurs enfants.\"]\n",
            "['-- Chartait de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de son de s']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 2.\n",
            "Average loss: 1.956114400406273.\n",
            "131.60618233680725 s elapsed (i.e. 43.86872744560242 s/epoch)\n",
            "[\"Tous les corps animés reçoivent et communiquent l'influence des astres, propriété analogue à la vertu de l'aimant. En dirigeant cette force on peut guérir les malades, voilà le principe. La science, depuis Mesmer, s'est développée;--mais il importe toujours de verser le fluide et de faire des passes qui, premièrement, doivent endormir.\"]\n",
            "['-- Charre de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 3.\n",
            "Average loss: 1.8335126309978718.\n",
            "176.51902747154236 s elapsed (i.e. 44.12975686788559 s/epoch)\n",
            "['--Pardon! je vous rappellerai Hypatie, Jérôme de Prague, Jean Huss, Bruno, Vanini, Anne Du Bourg!']\n",
            "['-- Celle se comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 4.\n",
            "Average loss: 1.7468308313768737.\n",
            "219.5264711380005 s elapsed (i.e. 43.905294227600095 s/epoch)\n",
            "[\"Pour la juger impartialement, il faudrait avoir lu toutes les histoires, tous les mémoires, tous les journaux et toutes les pièces manuscrites, car de la moindre omission une erreur peut dépendre qui en amènera d'autres à l'infini. Ils y renoncèrent.\"]\n",
            "['-- Charles de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureux de la coureu']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 5.\n",
            "Average loss: 1.6721891748661897.\n",
            "264.76193356513977 s elapsed (i.e. 44.12698892752329 s/epoch)\n",
            "[\"Pécuchet qui s'était absenté une minute, lui glissa dans la main un napoléon.\"]\n",
            "['-- Ah! comme de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la courier de la cour']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 6.\n",
            "Average loss: 1.6040914415708047.\n",
            "309.0483515262604 s elapsed (i.e. 44.149764503751484 s/epoch)\n",
            "['--Est-ce vrai? dit-elle tu vas te battre?']\n",
            "['-- Ah! se rentrait de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de la porte de']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 7.\n",
            "Average loss: 1.5447282158598608.\n",
            "353.2753965854645 s elapsed (i.e. 44.15942457318306 s/epoch)\n",
            "[\"Marescot, homme de goût, riposta que Molière ne passerait plus--et d'ailleurs était un peu surfait.\"]\n",
            "[\"-- Ce n'est pas de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure de la considure d\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 8.\n",
            "Average loss: 1.4947911482684466.\n",
            "399.68885374069214 s elapsed (i.e. 44.40987263785468 s/epoch)\n",
            "[\"Au second tournant, quand il aperçut le vide, la peur le glaça. À mesure qu'il approchait du troisième, ses jambes devenaient molles. Les couches de l'air vibraient autour de lui, une crampe le pinçait à l'épigastre; il s'assit par terre les yeux fermés, n'ayant plus conscience que des battements de son coeur qui l'étouffaient. Puis, il jeta son bâton de touriste, et avec les genoux et les mains reprit son ascension. Mais les trois marteaux tenus à la ceinture lui entraient dans le ventre, les cailloux dont ses poches étaient bourrées tapaient ses flancs; la visière de sa casquette l'aveuglait, le vent redoublait de force; enfin il atteignit le plateau et y trouva Bouvard qui était monté plus loin, par une valleuse moins difficile.\"]\n",
            "['-- Ah! mon de son petit de la conserver les paresses de la conserver les paresses de la conserver les paresses de la conserver les paresses de la conserver les paresses de la conserver les paresses de la conserver les paresses de la conserver les paresses de la conserver les paresses de la conserver les paresses de la conserver les paresses de la conserver les paresses de la conserver les paresses de la conserver les paresses de la conserver les paresses de la conserver les paresses de la conserver les pares']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 9.\n",
            "Average loss: 1.449630051243062.\n",
            "442.4793791770935 s elapsed (i.e. 44.24793791770935 s/epoch)\n",
            "[\"Avant l'heure de la cérémonie, tous les trois attendaient le cortège.\"]\n",
            "['-- Ah! mon est pas de la contre les parties de la contre les parties de la contre les parties de la contre les parties de la contre les parties de la contre les parties de la contre les parties de la contre les parties de la contre les parties de la contre les parties de la contre les parties de la contre les parties de la contre les parties de la contre les parties de la contre les parties de la contre les parties de la contre les parties de la contre les parties de la contre les parties de la contre les pa']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 10.\n",
            "Average loss: 1.4233096302772055.\n",
            "486.4831256866455 s elapsed (i.e. 44.22573869878595 s/epoch)\n",
            "[\"Au galop de quatre chevaux, elle était emportée depuis huit jours vers un pays nouveau, d'où ils ne reviendraient plus. Ils allaient, ils allaient, les bras enlacés, sans parler. Souvent, du haut d'une montagne, ils apercevaient tout à coup quelque cité splendide avec des dômes, des ponts, des navires, des forêts de citronniers et des cathédrales de marbre blanc, dont les clochers aigus portaient des nids de cigogne. On marchait au pas, à cause des grandes dalles, et il y avait par terre des bouquets de fleurs que vous offraient des femmes habillées en corset rouge. On entendait sonner des cloches, hennir les mulets, avec le murmure des guitares et le bruit des fontaines, dont la vapeur s'envolant rafraîchissait des tas de fruits, disposés en pyramide au pied des statues pâles, qui souriaient sous les jets d'eau. Et puis ils arrivaient, un soir, dans un village de pêcheurs, où des filets bruns séchaient au vent, le long de la falaise et des cabanes. C'est là qu'ils s'arrêteraient pour vivre; ils habiteraient une maison basse, à toit plat, ombragée d'un palmier, au fond d'un golfe, au bord de la mer. Ils se promèneraient en gondole, ils se balanceraient en hamac; et leur existence serait facile et large comme leurs vêtements de soie, toute chaude et étoilée comme les nuits douces qu'ils contempleraient. Cependant, sur l'immensité de cet avenir qu'elle se faisait apparaître, rien de particulier ne surgissait; les jours, tous magnifiques, se ressemblaient comme des flots; et cela se balançait à l'horizon, infini, harmonieux, bleuâtre et couvert de soleil. Mais l'enfant se mettait à tousser dans son berceau, ou bien Bovary ronflait plus fort, et Emma ne s'endormait que le matin, quand l'aube blanchissait les carreaux et que déjà le petit Justin, sur la place, ouvrait les auvents de la pharmacie.\"]\n",
            "['-- Ah! ce qui se reprit le première de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la construment de la con']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 11.\n",
            "Average loss: 1.3866186829245821.\n",
            "531.6497321128845 s elapsed (i.e. 44.30414434274038 s/epoch)\n",
            "[\"-- Parle-nous! disait Charles, parle-nous! Remets-toi! C'est moi, ton Charles qui t'aime! Me reconnais-tu? Tiens, voilà ta petite fille: embrasse-la donc!\"]\n",
            "['-- Ah! mais le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprendre le comprend']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 12.\n",
            "Average loss: 1.3590271436623511.\n",
            "576.3105347156525 s elapsed (i.e. 44.33157959351173 s/epoch)\n",
            "[\"Ce qui allait se passer tout à l'heure était inexplicable, songeait Bouvard; mais la Raison ne suffit pas à comprendre certaines choses. De très grands hommes ont admis celle-là. Autant faire comme eux. Et dans une sorte d'engourdissement, il contemplait l'autel, l'encensoir, les flambeaux, la tête un peu vide car il n'avait rien mangé--et éprouvait une singulière faiblesse.\"]\n",
            "['Le cours de la conseille de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la cours de la co']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 13.\n",
            "Average loss: 1.3340959749659713.\n",
            "619.7576920986176 s elapsed (i.e. 44.268406578472685 s/epoch)\n",
            "['-- Se moque-t-elle de moi? songeait Rodolphe.']\n",
            "[\"-- Ce n'est pas de la considération de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l'autre de l\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 14.\n",
            "Average loss: 1.3124877171857017.\n",
            "664.5351929664612 s elapsed (i.e. 44.30234619776408 s/epoch)\n",
            "[\"Et madame Bovary, non plus que Rodolphe, ne lui répondait guère, tandis qu'au moindre mouvement qu'ils faisaient, il se rapprochait en disant: «Plaît-il?» et portait la main à son chapeau.\"]\n",
            "[\"-- Ah! c'est que le cours de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chamb\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 15.\n",
            "Average loss: 1.292150726123732.\n",
            "709.1338603496552 s elapsed (i.e. 44.32086627185345 s/epoch)\n",
            "[\"Alors, pour le distraire, Homais jugea convenable de causer un peu horticulture; les plantes avaient besoin d'humidité. Charles baissa la tête en signe d'approbation.\"]\n",
            "[\"-- Ah! c'est un part des paroles de la chambre des choses de la chambre des choses de la chambre des choses de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscience de la conscien\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 16.\n",
            "Average loss: 1.273576196967339.\n",
            "751.9732985496521 s elapsed (i.e. 44.23372344409718 s/epoch)\n",
            "['Et il reprit']\n",
            "['-- Eh bien! reprit le mariage de la contendre de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la cour de la']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 17.\n",
            "Average loss: 1.2617684700051133.\n",
            "795.563481092453 s elapsed (i.e. 44.19797117180295 s/epoch)\n",
            "['-- Oui, je sais..., je sais...']\n",
            "[\"-- Ah! je vous ai pas de la considération de l'autre partie de l'autre partie de la conseiller des considérations de la considération de la considération de la considération de la considération de la considération de l'autre par les propriétés de la considération de la considération de la considération de la considération de la considération de l'autre par les propriétés de la considération de la considération de la considération de la considération de la considération de l'autre par les propriétés de la con\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 18.\n",
            "Average loss: 1.2515392881266925.\n",
            "838.1243674755096 s elapsed (i.e. 44.111808814500506 s/epoch)\n",
            "['Pécuchet exprima son horreur des filles publiques.']\n",
            "[\"-- Ah! ce n'est pas la maison.\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 19.\n",
            "Average loss: 1.2373370815654696.\n",
            "881.1296257972717 s elapsed (i.e. 44.056481289863584 s/epoch)\n",
            "[\"Un cheval de voltige en bois avec le rembourrage eût été dispendieux, ils y renoncèrent; le tilleul abattu dans le jardin leur servit de mât horizontal; et quand ils furent habiles à le parcourir d'un bout à l'autre, pour en avoir un vertical, ils replantèrent une poutrelle des contre-espaliers. Pécuchet gravit jusqu'en haut. Bouvard glissait, retombait toujours, finalement, y renonça.\"]\n",
            "['-- Eh bien, dit le monde de la main sur le contraire, et les promes de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la conservation de la co']\n",
            "BLEU : 0.0\n",
            "lr,emb,hid,lay,B :  (0.02, 128, 512, 1, 64)\n",
            "-- END OF EPOCH 0.\n",
            "Average loss: 3.285970444631095.\n",
            "38.89782667160034 s elapsed (i.e. 38.89782667160034 s/epoch)\n",
            "[\"--Ah! tu n'es qu'un sophiste! Et Pécuchet, vexé, bouda pendant trois jours.\"]\n",
            "[' le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 1.\n",
            "Average loss: 2.4825363524106083.\n",
            "79.04050803184509 s elapsed (i.e. 39.520254015922546 s/epoch)\n",
            "['La voici:']\n",
            "['---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 2.\n",
            "Average loss: 2.2677149091448103.\n",
            "118.2333550453186 s elapsed (i.e. 39.41111834843954 s/epoch)\n",
            "['Pourquoi leur bonne semblait-elle en avoir peur?']\n",
            "['-- le de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 3.\n",
            "Average loss: 2.1182126317705428.\n",
            "156.55170154571533 s elapsed (i.e. 39.13792538642883 s/epoch)\n",
            "[\"Elle avait lu Paul et Virginie et elle avait rêvé la maisonnette de bambous, le nègre Domingo, le chien Fidèle, mais surtout l'amitié douce de quelque bon petit frère, qui va chercher pour vous des fruits rouges dans des grands arbres plus hauts que des clochers, ou qui court pieds nus sur le sable, vous apportant un nid d'oiseau.\"]\n",
            "[\"-- l'aut les de se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la se la\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 4.\n",
            "Average loss: 2.0069818545360953.\n",
            "195.85325241088867 s elapsed (i.e. 39.170650482177734 s/epoch)\n",
            "[\"Victor comme un soldat, s'était mis son bagage sur le dos. Il sifflait--jetait des pierres aux corneilles dans les sillons, allait sous les arbres, pour se couper des badines--Foureau le rappela; et Bouvard, en le retenant par la main jouissait de sentir dans la sienne ces doigts d'enfant robustes et vigoureux. Le pauvre petit diable ne demandait qu'à se développer librement, comme une fleur en plein air! et il pourrirait entre des murs avec des leçons, des punitions, un tas de bêtises! Bouvard fut saisi par une révolte de la pitié, une indignation contre le sort, une de ces rages où l'on veut détruire le gouvernement.\"]\n",
            "['-- Le comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se comme de se com']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 5.\n",
            "Average loss: 1.9284254945054347.\n",
            "235.45246529579163 s elapsed (i.e. 39.24207754929861 s/epoch)\n",
            "[\"Le choix l'embarrassait. Elle n'avait vu que trois pièces: _Robert le Diable_ dans la capitale, le _Jeune Mari_ à Rouen--et une autre à Falaise qui était bien amusante et qu'on appelait _La Brouette du Vinaigrier_.\"]\n",
            "['-- Elle se partion de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 6.\n",
            "Average loss: 1.8651004348482405.\n",
            "275.84664845466614 s elapsed (i.e. 39.4066640649523 s/epoch)\n",
            "[\"Bouvard dans son empressement faillit renverser Mme Bordin qui se trouvait là. Puis, apercevant un de ses valets, il l'accabla d'injures pour ne l'avoir pas averti. Le valet au contraire, par excès de zèle avait d'abord couru à la maison, à l'église, puis chez Monsieur, et était revenu par l'autre route.\"]\n",
            "['-- Charres de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comme de la comm']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 7.\n",
            "Average loss: 1.812847349108482.\n",
            "316.3700315952301 s elapsed (i.e. 39.54625394940376 s/epoch)\n",
            "['La première fut de tirer du caillou.']\n",
            "['-- Ce la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de la charme de ']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 8.\n",
            "Average loss: 1.763841790812356.\n",
            "356.9084298610687 s elapsed (i.e. 39.656492206785416 s/epoch)\n",
            "[\"Et il agita son journal en les regardant s'éloigner.\"]\n",
            "['-- Ce la pour le proses de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la mais de la']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 9.\n",
            "Average loss: 1.7247457333973475.\n",
            "395.6784415245056 s elapsed (i.e. 39.56784415245056 s/epoch)\n",
            "[\"Pécuchet répliqua qu'ils n'étaient pas malades, et ayant exposé le but de leur visite:\"]\n",
            "['-- Comme un pour le contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 10.\n",
            "Average loss: 1.6853861103252488.\n",
            "436.3691625595093 s elapsed (i.e. 39.6699238690463 s/epoch)\n",
            "['-- Eh bien? répondit-elle.']\n",
            "['-- Comme un contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre de la contre ']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 11.\n",
            "Average loss: 1.6500385427961544.\n",
            "476.2641282081604 s elapsed (i.e. 39.68867735068003 s/epoch)\n",
            "[\"Aucun des deux n'avait caché à l'autre son opinion. Chacun en reconnut la justesse. Leurs habitudes changèrent; et quittant leur pension bourgeoise, ils finirent par dîner ensemble tous les jours.\"]\n",
            "['-- Ah! montre le charles de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambr']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 12.\n",
            "Average loss: 1.6137401877027568.\n",
            "515.9626128673553 s elapsed (i.e. 39.689431759027336 s/epoch)\n",
            "[\"Elle éclata en sanglots. Rodolphe crut que c'était l'explosion de son amour; comme elle se taisait, il prit ce silence pour une dernière pudeur, et alors il s'écria:\"]\n",
            "['-- Ce me des consieurs de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles de la charles ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 13.\n",
            "Average loss: 1.5804815547806876.\n",
            "554.9690322875977 s elapsed (i.e. 39.64064516339983 s/epoch)\n",
            "[\"-- C'est une chose si maussade, soupira le clerc, que de vivre cloué aux mêmes endroits!\"]\n",
            "[\"-- Ah! n'en avait pas de la cour de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur de la courrieur \"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 14.\n",
            "Average loss: 1.547404101916722.\n",
            "595.8603854179382 s elapsed (i.e. 39.72402569452922 s/epoch)\n",
            "['--Ma foi! prenez-les! dit Foureau.']\n",
            "['-- Ah! mais le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre l']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 15.\n",
            "Average loss: 1.5181623149891288.\n",
            "636.8275465965271 s elapsed (i.e. 39.801721662282944 s/epoch)\n",
            "[\"Emma ne répondit rien. Elle haletait, tout en roulant les yeux autour d'elle, tandis que la paysanne, effrayée de son visage, se reculait instinctivement, la croyant folle. Tout à coup elle se frappa le front, poussa un cri, car le souvenir de Rodolphe, comme un grand éclair dans une nuit sombre, lui avait passé dans l'âme. Il était si bon, si délicat, si généreux! Et, d'ailleurs, s'il hésitait à lui rendre ce service, elle saurait bien l'y contraindre en rappelant d'un seul clin d'oeil leur amour perdu. Elle partit donc vers la Huchette, sans s'apercevoir qu'elle courait s'offrir à ce qui l'avait tantôt si fort exaspérée, ni se douter le moins du monde de cette prostitution.\"]\n",
            "[\"-- Ah! n'est pas de la chambre de la contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre le contre\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 16.\n",
            "Average loss: 1.4956360325521352.\n",
            "676.6746778488159 s elapsed (i.e. 39.80439281463623 s/epoch)\n",
            "[\"Elle fit, au commencement du printemps, bouleverser le jardin d'un bout à l'autre, malgré les observations de Bovary; il fut heureux, cependant de lui voir enfin manifester une volonté quelconque. Elle en témoigna davantage à mesure qu'elle se rétablissait. D'abord, elle trouva moyen d'expulser la mère Rolet, la nourrice, qui avait pris l'habitude, pendant sa convalescence, de venir trop souvent à la cuisine avec ses deux nourrissons et son pensionnaire, plus endenté qu'un cannibale. Puis elle se dégagea de la famille Homais, congédia successivement toutes les autres visites et même fréquenta l'église avec moins d'assiduité, à la grande approbation de l'apothicaire, qui lui dit alors amicalement:\"]\n",
            "[\"-- Ah! n'est pas de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la chambre de la \"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 17.\n",
            "Average loss: 1.4643688153247445.\n",
            "716.4121894836426 s elapsed (i.e. 39.8006771935357 s/epoch)\n",
            "[\"Emma, en face de lui, le regardait; elle ne partageait pas son humiliation, elle en éprouvait une autre: c'était de s'être imaginé qu'un pareil homme pût valoir quelque chose, comme si vingt fois déjà elle n'avait pas suffisamment aperçu sa médiocrité.\"]\n",
            "[\"-- Ce n'est pas de la conseille de la conserve de la conseille de la conserve de la conseille de la conserve de la conseille de la conserve de la conseille de la conserve de la conseille de la conserve de la conseille de la conserve de la conseille de la conserve de la conseille de la conserve de la conseille de la conserve de la conseille de la conserve de la conseille de la conserve de la conseille de la conserve de la conseille de la conserve de la conseille de la conserve de la conseille de la conserve d\"]\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 18.\n",
            "Average loss: 1.4483148516440878.\n",
            "754.997460603714 s elapsed (i.e. 39.73670845282705 s/epoch)\n",
            "[\"M. Rodolphe Boulanger avait trente-quatre ans; il était de tempérament brutal et d'intelligence perspicace, ayant d'ailleurs beaucoup fréquenté les femmes, et s'y connaissant bien. Celle-là lui avait paru jolie; il y rêvait donc, et à son mari.\"]\n",
            "['-- Ah! mais le conseillait de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la conserve de la ']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 19.\n",
            "Average loss: 1.4230686778924904.\n",
            "794.5005674362183 s elapsed (i.e. 39.725028371810915 s/epoch)\n",
            "[\"Depuis le jour où Pécuchet avait observé la petite bonne tirant de l'eau il lui parlait plus souvent;--et soit qu'elle balayât le corridor, ou qu'elle étendit du linge, ou qu'elle tournât les casseroles, il ne pouvait se rassasier du bonheur de la voir,--surpris lui-même de ses émotions, comme dans l'adolescence. Il en avait les fièvres et les langueurs,--et était persécuté par le souvenir de Mme Castillon, étreignant Gorju.\"]\n",
            "['Le contrait de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération de la considération d']\n",
            "BLEU : 0.0\n",
            "lr,emb,hid,lay,B :  (0.02, 128, 512, 2, 32)\n",
            "-- END OF EPOCH 0.\n",
            "Average loss: 3.2759698671737905.\n",
            "87.47195959091187 s elapsed (i.e. 87.47195959091187 s/epoch)\n",
            "[\"-- Comment voulais-tu que je vécusse sans toi? On ne peut pas se déshabituer du bonheur! J'étais désespérée! j'ai cru mourir! Je te conterai tout cela, tu verras. Et toi... tu m'as fuie!...\"]\n",
            "['- le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le le l']\n",
            "BLEU : 0.0\n",
            "-- END OF EPOCH 1.\n",
            "Average loss: 2.4844648838043213.\n",
            "173.9768421649933 s elapsed (i.e. 86.98842108249664 s/epoch)\n",
            "[\"La Raison vous dit: Trois c'est trois--et la Foi déclare que: Trois c'est un.\"]\n",
            "['Le le sa cour de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se coure de se c']\n",
            "BLEU : 0\n",
            "-- END OF EPOCH 2.\n",
            "Average loss: 2.1640088120285346.\n",
            "259.65888357162476 s elapsed (i.e. 86.55296119054158 s/epoch)\n",
            "['-- Je vous en conjure, monsieur Lheureux, quelques jours encore!']\n",
            "['-- Au les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des les pars des ']\n",
            "BLEU : 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-04b78f701be9>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_paragraphs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 492\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    493\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    252\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "UC-fETUtG_9X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iLgBHtKTagR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_toy_dataset = True"
      ],
      "metadata": {
        "id": "7xAyZnZCRIlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxs4UZRAHxx3"
      },
      "source": [
        "if(use_toy_dataset):\n",
        "  prompt = [\"AAAA. Now, please write 3 i.\" + EOP] #[paragraphs[0]]\n",
        "  print(prompt[0])\n",
        "  print(f\"(Is this prompt in the training set? {prompt[0] in paragraphs})\\n\")\n",
        "\n",
        "  for _ in range(10):\n",
        "    batch = batch_generator.turn_into_batch(prompt)\n",
        "    gen_texts = model.predictionStrings(batch.to(model.device), max_predicted_char=128)\n",
        "\n",
        "    print(gen_texts[0])\n",
        "    prompt = [gen_texts[0] + EOP]\n",
        "else:\n",
        "  prompt = [\"Charles Bovary sortit une bonne bouteille de vin et alla chercher des verres pour ses invités.\"] * 10\n",
        "  batch = batch_generator.turn_into_batch(prompt)\n",
        "  gen_texts = model.predictionStrings(batch.to(model.device), max_predicted_char=1024)\n",
        "\n",
        "  print(prompt[0])\n",
        "  print()\n",
        "  for i, gen_text in enumerate(gen_texts):\n",
        "    print(f\"{i}: \", end=\"\")\n",
        "    print(gen_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your system does not work as expected, check that you are using a sensible loss function, but also check that your implementation matches the architecture depicted in https://moodle.u-paris.fr/mod/resource/view.php?id=648001.\n",
        "\n",
        "If you cannot get your model to work even on the toy dataset, then there must be a bug somewhere."
      ],
      "metadata": {
        "id": "50HbcnPmXkMT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J2GHjtPr4Mw2"
      },
      "source": [
        "Read the remarks at the beginning of the TP again.\n",
        "\n",
        "Once you are sure that your system is correctly implemented and generates texts that look a little bit like natural language, find ways to improve the system.\n",
        "Here are some ideas (ordered arbitrarily):\n",
        "\n",
        "*   Compute a measure that evaluates the performance of the model.\n",
        "*   Split your dataset into a training and a development section, and use this split in a relevant way.\n",
        "*   Implement beam decoding instead of greedy decoding.\n",
        "*   Use other units of text instead of characters (ex: words, word-pieces).\n",
        "*   Add more data to the dataset.\n",
        "*   Use graphs to visualise the training process and the predictions.\n",
        "\n",
        "Document in a text cell all of the changes that you make to the system and describe their impact (qualitatively **and** quantitatively)."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aKa8b0Q0XdmR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}