{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "import os\n",
    "import torch\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =\"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-large\" \n",
    "# model_name = \"google/flan-t5-xl\"\n",
    "directory = model_name.split(\"/\")[1]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping dataset names to file paths and additional parameters\n",
    "\n",
    "dataset_map = {\n",
    "    \"SX\": { # sexism data of full train set \n",
    "        \"filepath\": \"../../../data/sexism_dataset_3_classes_without_duplicates_train.csv\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"usecols\": [\"label\", \"toxicity\", \"sentence\"]\n",
    "    },\n",
    "    \"SXM\": { # sexism data of 50 most toxic ex\n",
    "        \"filepath\": \"../../../data/output_most_toxic_2024_06_11_53_sentences.csv\",\n",
    "        \"sep\": \",\",\n",
    "        \"usecols\": [\"label\", \"toxicity\", \"sentence\"]\n",
    "    },\n",
    "    \"SXL\": { # sexism data of 50 least toxic ex\n",
    "        \"filepath\": \"../../../data/output_least_toxic_2024_06_11_53_sentences.csv\",\n",
    "        \"sep\": \",\",\n",
    "        \"usecols\": [\"label\", \"toxicity\", \"sentence\"]\n",
    "    },\n",
    "    \"CN\": { # conan(islamophbic) data\n",
    "        \"filepath\": \"../../../data/conan_train.csv\",\n",
    "        \"sep\": \"\\t\",\n",
    "        \"usecols\": None\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_dataset(dataset_name, head=None):\n",
    "    config = dataset_map[dataset_name]\n",
    "    data = pd.read_csv(config[\"filepath\"], sep=config[\"sep\"], usecols=config[\"usecols\"])\n",
    "    if dataset_name != \"CN\":\n",
    "        data.rename(columns={\"label\": \"gold\", \"sentence\": \"text\"}, inplace=True)\n",
    "        # data = data.head(head)    \n",
    "    return data\n",
    "\n",
    "dataset_name=\"SXL\"\n",
    "data = load_dataset(dataset_name)\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_prompt_template(prompt_template_filepath) -> dict: \n",
    "    with open(prompt_template_filepath, 'r') as file:\n",
    "        content = file.read()\n",
    "    # Use regular expressions to find keys and their corresponding values\n",
    "    pattern = re.compile(r'(\\w+):\\n(.*?)\\n\\n', re.DOTALL)\n",
    "    matches = pattern.findall(content)\n",
    "    prompt_template_from_file = {key: value.strip() for key, value in matches}\n",
    "    return prompt_template_from_file\n",
    "\n",
    "prompt_templates = read_prompt_template(\"../../../main_prompts.txt\")\n",
    "\n",
    "def charge_example(prompt_template,texts): # charge sentences to prompt \n",
    "  prompts_charged = [prompt_template.format(sent=text) for text in texts]\n",
    "  return prompts_charged\n",
    "\n",
    "def turn_into_batch(prompts, batch_size):\n",
    "  batch_idx = 0\n",
    "  for i in range(0,len(prompts),batch_size):\n",
    "    yield prompts[i:i+batch_size]\n",
    "    batch_idx+=1\n",
    "    print(f\"Processing {batch_idx}th batch\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_args = {\n",
    "    \"max_length\":10000,\n",
    "    # \"max_new_tokens\": 256,\n",
    "    # \"do_sample\": True,\n",
    "    \"temperature\": 0.01,\n",
    "    \"top_p\": 0.9,\n",
    "    \"top_k\": 1,\n",
    "    # \"repetition_penalty\": 1.05,\n",
    "    # \"eos_token_id\": [tokenizer.eos_token_id, 3200],\n",
    "    # \"output_logits\": True,\n",
    "    # \"return_dict_in_generate\": True,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "    \n",
    "def inference(batch_of_prompts) -> list :\n",
    "  inputs = tokenizer(batch_of_prompts, return_tensors=\"pt\", padding=True).to(device)\n",
    "  outputs = model.generate(**inputs, **generation_args)\n",
    "  decoded_outputs = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
    "  return decoded_outputs #output for a batch in a ls\n",
    "\n",
    "def store_result(out, prompting_type, directory, data, dataset_name):\n",
    "  res_filename = directory + f\"/TEST{dataset_name}_{len(out)}_{prompting_type}.csv\"\n",
    "  res = pd.concat([data, pd.DataFrame(out, columns=[\"pred\"])], axis=1)\n",
    "  res.to_csv(res_filename,index=False)\n",
    "  print(res_filename)\n",
    "  return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference over all types of prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterhub-users/elee/.eunkyung_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jupyterhub-users/elee/.eunkyung_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyterhub-users/elee/.eunkyung_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:537: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1th batch\n",
      "['NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'YES', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO', 'NO']\n"
     ]
    }
   ],
   "source": [
    "for prompting_type, prompt_template in prompt_templates.items():\n",
    "  out = []\n",
    "  prompts = charge_example(prompt_template,data[\"text\"])\n",
    "  out = [inf for batch in turn_into_batch(prompts,batch_size=500) for inf in inference(batch)]\n",
    "  # res = store_result(out, prompting_type, directory)\n",
    "  print(out)\n",
    "  break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flan-t5-large/prompts.txt\n"
     ]
    }
   ],
   "source": [
    "model_name = \"google/flan-t5-large\" \n",
    "directory = model_name.split(\"/\")[1]\n",
    "prompt_templates = read_prompt_template(directory+\"/prompts.txt\")\n",
    "print(directory+\"/prompts.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterhub-users/elee/.eunkyung_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.01` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/jupyterhub-users/elee/.eunkyung_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:520: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/home/jupyterhub-users/elee/.eunkyung_venv/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:537: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1th batch\n",
      "Processing 2th batch\n",
      "Processing 3th batch\n",
      "Processing 4th batch\n",
      "Processing 5th batch\n",
      "Processing 6th batch\n",
      "Processing 7th batch\n",
      "Processing 8th batch\n",
      "Processing 9th batch\n",
      "Processing 10th batch\n",
      "Processing 11th batch\n",
      "flan-t5-large/TESTCN_874_P1EN_binary_v1.csv\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to test a prompt type chosen.\"\"\"\n",
    "    \n",
    "    # Choose model size, daataset, prompting type\n",
    "    model_name = \"google/flan-t5-large\" \n",
    "    # model_name = \"google/flan-t5-xl\" \n",
    "    dataset_name=\"CN\"\n",
    "    prompting_type = \"P1EN_binary_v1\"\n",
    "\n",
    "    data = load_dataset(dataset_name)\n",
    "    directory = model_name.split(\"/\")[1]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "    prompt_templates = read_prompt_template(directory+\"/prompts.txt\")\n",
    "    prompts = charge_example(prompt_templates[prompting_type], data[\"text\"])\n",
    "\n",
    "    out = []\n",
    "    for batch in turn_into_batch(prompts, batch_size=int(len(prompts) / 10)):\n",
    "      out.extend(inference(batch))   \n",
    "    res = store_result(out, prompting_type, directory, data, dataset_name)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
